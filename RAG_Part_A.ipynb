{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pld0aGdIBVhZ"
   },
   "source": [
    "**Homework 12** - **Part A: Code Chatbot**\n",
    "\n",
    "**Building RAG Chatbots with LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3xE-W5ew4wU"
   },
   "source": [
    "### Part A Goal\n",
    "\n",
    "Build a code understanding model using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using **Retrieval Augmented Generation (RAG)**.\n",
    "\n",
    "Uploading previous assignment notebook files to the model we will be able to ask questions based on the code file as context.\n",
    "\n",
    "This example will have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg8_mMrYw4wV"
   },
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnRFn2gdw4wV"
   },
   "source": [
    "Install the following Python libraries:\n",
    "\n",
    "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
    "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
    "- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n",
    "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
    "\n",
    "**NOTE**: *OpenAI dataloaders will not load locally for on-prem devices easily. To simplify the use of these loaders, it is recommended to use an online notebook such as CoLab.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CflZ3e82w4wV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.0.354 \\\n",
    "    openai==1.6.1 \\\n",
    "    datasets==2.10.1 \\\n",
    "    pinecone-client==3.1.0 \\\n",
    "    tiktoken==0.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUH6ontXw4wV"
   },
   "source": [
    "### BACKGROUND: Building a Chatbot (no RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAps57bQw4wW"
   },
   "source": [
    "We will be using Langchain library for our chatbot. We initialize `ChatOpenAI` object using the *OPENAI_KEY*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hF4HeBjw4wW",
    "outputId": "64da795f-467c-4728-95a3-d71b984867e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax2S29rZw4wW"
   },
   "source": [
    "Chat models are usually structured in plain text in the below format\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In LangChain there is a slightly different format. We use three _message_ objects like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kSbM7Phiw4wW"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful expert data scientist python coder.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand pytorch and Langchain in two paragraphs.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6TnpBRAw4wW"
   },
   "source": [
    "The format is very similar, we're just swapping the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHFShuVMw4wW",
    "outputId": "06038913-1d00-452c-e0be-04bbd995a179"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='PyTorch is an open-source machine learning library for Python that provides a flexible and dynamic computational graph system, which allows for easy and efficient building of deep learning models. It offers a wide range of tools and modules that make it easier to create and train neural networks, such as layers, optimizers, and loss functions. PyTorch is known for its simplicity and ease of use, making it a popular choice among researchers and practitioners in the deep learning community.\\n\\nLangchain, on the other hand, is a domain-specific language (DSL) designed for creating and training neural networks. It aims to simplify the process of developing deep learning models by providing a high-level abstraction that allows users to define their models using a more human-readable syntax. Langchain abstracts away the complexities of low-level programming and focuses on the essentials of building neural networks, making it easier for developers to experiment with different architectures and algorithms. By combining the power of a DSL like Langchain with the flexibility of a deep learning framework like PyTorch, developers can accelerate the development and deployment of cutting-edge AI applications.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7LoZQClw4wW"
   },
   "source": [
    "In response we get another AI message object. We can print it more clearly like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DzT8TK29w4wX",
    "outputId": "cd87c6f5-b6be-4950-b530-5f63df71f2bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is an open-source machine learning library for Python that provides a flexible and dynamic computational graph system, which allows for easy and efficient building of deep learning models. It offers a wide range of tools and modules that make it easier to create and train neural networks, such as layers, optimizers, and loss functions. PyTorch is known for its simplicity and ease of use, making it a popular choice among researchers and practitioners in the deep learning community.\n",
      "\n",
      "Langchain, on the other hand, is a domain-specific language (DSL) designed for creating and training neural networks. It aims to simplify the process of developing deep learning models by providing a high-level abstraction that allows users to define their models using a more human-readable syntax. Langchain abstracts away the complexities of low-level programming and focuses on the essentials of building neural networks, making it easier for developers to experiment with different architectures and algorithms. By combining the power of a DSL like Langchain with the flexibility of a deep learning framework like PyTorch, developers can accelerate the development and deployment of cutting-edge AI applications.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXb4dFHXw4wX"
   },
   "source": [
    "### Stringing Messages for a Conversation\n",
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UZVj7P5w4wX",
    "outputId": "6a201108-c695-4e74-e0d6-abcd9ba50c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scientists believe that artificial intelligence (AI) has the potential to achieve general intelligence due to its ability to learn from large amounts of data and adapt to new tasks and environments. Machine learning algorithms, such as deep learning models, have shown impressive capabilities in performing complex tasks, such as image recognition, natural language processing, and game playing, which were previously thought to require human-level intelligence.\n",
      "\n",
      "Moreover, the advancements in AI research, particularly in areas like reinforcement learning, transfer learning, and meta-learning, have shown promising results in enabling AI systems to generalize their knowledge and skills across different domains. By leveraging these techniques and combining them with large-scale data and computational resources, data scientists believe that it is possible to develop AI systems that exhibit human-like intelligence and can perform a wide range of cognitive tasks.\n",
      "\n",
      "While achieving general artificial intelligence remains a long-term goal with many technical and ethical challenges to overcome, data scientists are optimistic about the potential of AI to revolutionize various industries and solve complex problems in ways that were previously unimaginable.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do data scientists believe it can produce general artificial intelligence?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtSC6WmBw4wX"
   },
   "source": [
    "### Dealing with Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAhStCG9w4wX"
   },
   "source": [
    "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about the new (and very popular) Llama 3 LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vdtjTQebw4wX"
   },
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Llama 3?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zMi7B6LZw4wX",
    "outputId": "1def4ed1-62b0-428a-d19d-d9106423ab7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update, there is no specific information available about a technology or product called \"Llama 3.\" It is possible that it may refer to a new or upcoming technology, software, device, or concept that has been developed since then. If you have any specific details or context about Llama 3 that you would like me to explore further, please provide more information so that I can assist you better.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YORp9R_Pw4wX"
   },
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect.\n",
    "\n",
    "OpenAI have since adjusted the behavior for this particular example as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "30od2wnkw4wX"
   },
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you tell me more about the LLMChain in LangChain?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZOSGe-Kw4wX",
    "outputId": "02478bee-e3ac-4529-d0ee-ee308ba65313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the confusion earlier. As of my last update, there is no specific information available about a technology or concept called \"LLMChain\" in relation to LangChain or any other known system. It is possible that it may refer to a new or emerging concept in the field of blockchain or distributed ledger technology that has been developed since then. If you have any specific details or context about LLMChain in LangChain that you would like me to explore further, please provide more information so that I can assist you better.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma4gl6-rw4wX"
   },
   "source": [
    "### Feed the LLM More Data Manually [Not scalable]\n",
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HBpF2S0fw4wX"
   },
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Th7n7OZaw4wX"
   },
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Tq3IIKciw4wX"
   },
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAwwu54Ow4wX"
   },
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FnUzBUH0w4wX"
   },
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvYXrStmw4wX",
    "outputId": "669a5f05-8c25-4fe9-f636-f77a1583b6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LangChain, the LLMChain is a common type of chain that plays a crucial role in leveraging language models within the framework. The LLMChain comprises a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain is designed to handle multiple input variables, where the PromptTemplate is used to format these inputs into a prompt. The processed prompt is then passed to the model for generating an output. Additionally, the LLMChain can utilize an OutputParser, if provided, to further refine and parse the output generated by the language model into a final format. Overall, the LLMChain within LangChain facilitates the seamless integration of language models into applications, enhancing their functionality and capabilities in processing and interacting with data and environments.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNdgE8hZw4wY"
   },
   "source": [
    "The quality of this answer is phenomenal. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem — how do we get this information in the first place?\n",
    "\n",
    "We learned in the previous chapters about Pinecone and vector databases. Well, they can help us here too. But first, we'll need a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-JA-ut8w4wY"
   },
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oPwiUmbhBx8",
    "outputId": "c9c4e0a9-6bc2-4b74-ebf4-509905c688d0"
   },
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxc1ORMRf3qv",
    "outputId": "dea99557-fcb7-464a-e5e7-6c3e59e72ce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='AnbuV alluvan_Devadasan_Assignment9_T ransformer_German_to_English\\nMay 8, 2024\\nStep1. Run the demo and train a model on the original German-to-English training\\nset.\\n[1]:%matplotlib inline\\n1 Data Sourcing and Processing\\nW e will use Multi30k dataset from torchtext library that yields a pair of source-target raw sentences.\\nT o access torchtext datasets, please install torchdata following instructions at https://github.\\ncom/pytorch/data .\\n[2]: from torchtext .data .utils importget_tokenizer\\nfrom torchtext .vocab importbuild_vocab_from_iterator\\nfrom torchtext .datasets importmulti30k, Multi30k\\nfrom typing importIterable, List\\nmulti30k .URL[\"train\"]=\"https://raw.githubusercontent.com/neychev/\\n↪small_DL_repo/master/datasets/Multi30k/training.tar.gz \"\\nmulti30k .URL[\"valid\"]=\"https://raw.githubusercontent.com/neychev/\\n↪small_DL_repo/master/datasets/Multi30k/validation.tar.gz \"\\nSRC_LANGUAGE =\\'de\\'\\nTGT_LANGUAGE =\\'en\\'\\n# Place-holders\\ntoken_transform ={}\\nvocab_transform ={}\\n[3]:!pip install portalocker> =2.0.0\\n[4]: import torchdata\\n[5]: import portalocker\\n[6]: # !python -m spacy download en_core_web_sm\\n# !python -m spacy download de_core_news_sm\\n1', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 0}), Document(page_content='[7]:token_transform[SRC_LANGUAGE] =get_tokenizer( \\'spacy\\',␣\\n↪language =\\'de_core_news_sm \\')\\ntoken_transform[TGT_LANGUAGE] =get_tokenizer( \\'spacy\\',␣\\n↪language =\\'en_core_web_sm \\')\\n# helper function to yield list of tokens\\ndefyield_tokens (data_iter: Iterable, language: str)->List[str]:\\nlanguage_index ={SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\\nfordata_sample indata_iter:\\nyieldtoken_transform[language](data_sample[language_index[language]])\\n# Define special symbols and indices\\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX =0,1,2,3\\n# Make sure the tokens are in order of their indices to properly insert them in ␣\\n↪vocab\\nspecial_symbols =[\\'<unk>\\',\\'<pad>\\',\\'<bos>\\',\\'<eos>\\']\\n\"\"\"\\nPurpose: Defines special tokens used in the vocabulary for machine learning ␣\\n↪tasks with text data.\\nSpecial Tokens:\\n<unk>: \"Unknown\" token (represents words not in the vocabulary)\\n<pad>: Padding token (to make sequences the same length)\\n<bos>: \"Beginning of Sequence\"\\n<eos>: \"End of Sequence\"\\n\"\"\"\\nforln in[SRC_LANGUAGE, TGT_LANGUAGE]:\\n# Training data Iterator\\ntrain_iter =Multi30k(split =\\'train\\', language_pair =(SRC_LANGUAGE, ␣\\n↪TGT_LANGUAGE))\\n# Create torchtext\\'s Vocab object\\nvocab_transform[ln] =build_vocab_from_iterator(yield_tokens(train_iter, ␣\\n↪ln),\\nmin_freq =1,\\nspecials =special_symbols,\\nspecial_first =True)\\n# Set ``UNK_IDX`` as the default index. This index is returned when the token ␣\\n↪is not found.\\n# If not set, it throws ``RuntimeError`` when the queried token is not found in ␣\\n↪the Vocabulary.\\nforln in[SRC_LANGUAGE, TGT_LANGUAGE]:\\nvocab_transform[ln] .set_default_index(UNK_IDX)\\n2', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 1}), Document(page_content=\"[8]: from torch importTensor\\nimport torch\\nimport torch .nn as nn\\nfrom torch .nn importTransformer\\nimport math\\nDEVICE=torch.device('cuda' iftorch.cuda.is_available() else'cpu')\\n# helper Module that adds positional encoding to the token embedding to ␣\\n↪introduce a notion of word order.\\nclass PositionalEncoding (nn.Module):\\ndef__init__ (self,\\nemb_size: int,\\ndropout: float,\\nmaxlen: int=5000):\\nsuper(PositionalEncoding, self).__init__ ()\\nden=torch.exp(-torch.arange(0, emb_size, 2)*math.log(10000)/␣\\n↪emb_size)\\npos=torch.arange(0, maxlen) .reshape(maxlen, 1)\\npos_embedding =torch.zeros((maxlen, emb_size))\\npos_embedding[:, 0::2]=torch.sin(pos *den)\\npos_embedding[:, 1::2]=torch.cos(pos *den)\\npos_embedding =pos_embedding .unsqueeze( -2)\\nself.dropout =nn.Dropout(dropout)\\nself.register_buffer( 'pos_embedding ', pos_embedding)\\ndefforward(self, token_embedding: Tensor):\\nreturnself.dropout(token_embedding +self.pos_embedding[:\\n↪token_embedding .size(0), :])\\n# helper Module to convert tensor of input indices into corresponding tensor of ␣\\n↪token embeddings\\nclass TokenEmbedding (nn.Module):\\ndef__init__ (self, vocab_size: int, emb_size):\\nsuper(TokenEmbedding, self).__init__ ()\\nself.embedding =nn.Embedding(vocab_size, emb_size)\\nself.emb_size =emb_size\\ndefforward(self, tokens: Tensor):\\nreturnself.embedding(tokens .long()) *math.sqrt(self.emb_size)\\n# Seq2Seq Network\\nclass Seq2SeqTransformer (nn.Module):\\ndef__init__ (self,\\nnum_encoder_layers: int,\\nnum_decoder_layers: int,\\nemb_size: int,\\n3\", metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 2}), Document(page_content=\"nhead:int,\\nsrc_vocab_size: int,\\ntgt_vocab_size: int,\\ndim_feedforward: int=512,\\ndropout: float=0.1):\\nsuper(Seq2SeqTransformer, self).__init__ ()\\nself.transformer =Transformer(d_model =emb_size,\\nnhead=nhead,\\nnum_encoder_layers =num_encoder_layers,\\nnum_decoder_layers =num_decoder_layers,\\ndim_feedforward =dim_feedforward,\\ndropout=dropout)\\nself.generator =nn.Linear(emb_size, tgt_vocab_size)\\nself.src_tok_emb =TokenEmbedding(src_vocab_size, emb_size)\\nself.tgt_tok_emb =TokenEmbedding(tgt_vocab_size, emb_size)\\nself.positional_encoding =PositionalEncoding(\\nemb_size, dropout =dropout)\\ndefforward(self,\\nsrc: Tensor,\\ntrg: Tensor,\\nsrc_mask: Tensor,\\ntgt_mask: Tensor,\\nsrc_padding_mask: Tensor,\\ntgt_padding_mask: Tensor,\\nmemory_key_padding_mask: Tensor):\\nsrc_emb =self.positional_encoding( self.src_tok_emb(src))\\ntgt_emb =self.positional_encoding( self.tgt_tok_emb(trg))\\nouts=self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\\nsrc_padding_mask, tgt_padding_mask, ␣\\n↪memory_key_padding_mask)\\nreturnself.generator(outs)\\ndefencode(self, src: Tensor, src_mask: Tensor):\\nreturnself.transformer .encoder( self.positional_encoding(\\nself.src_tok_emb(src)), src_mask)\\ndefdecode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\\nreturnself.transformer .decoder( self.positional_encoding(\\nself.tgt_tok_emb(tgt)), memory,\\ntgt_mask)\\n[9]: defgenerate_square_subsequent_mask (sz):\\nmask=(torch.triu(torch .ones((sz, sz), device =DEVICE)) ==1).transpose( 0,␣\\n↪1)\\nmask=mask.float().masked_fill(mask ==0,float('-inf')).masked_fill(mask ␣\\n↪==1,float(0.0))\\n4\", metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 3}), Document(page_content='returnmask\\ndefcreate_mask (src, tgt):\\nsrc_seq_len =src.shape[0]\\ntgt_seq_len =tgt.shape[0]\\ntgt_mask =generate_square_subsequent_mask(tgt_seq_len)\\nsrc_mask =torch.zeros((src_seq_len, src_seq_len),device =DEVICE).type(torch .\\n↪bool)\\nsrc_padding_mask =(src==PAD_IDX) .transpose( 0,1)\\ntgt_padding_mask =(tgt==PAD_IDX) .transpose( 0,1)\\nreturnsrc_mask, tgt_mask, src_padding_mask, tgt_padding_mask\\nW e now define the parameters of our model and instantiate the same. Below, we also define our\\nloss function which is the cross-entropy loss and the optimizer used for training.\\n[10]:torch.manual_seed( 0)\\nSRC_VOCAB_SIZE =len(vocab_transform[SRC_LANGUAGE])\\nTGT_VOCAB_SIZE =len(vocab_transform[TGT_LANGUAGE])\\nEMB_SIZE =512\\nNHEAD=8\\nFFN_HID_DIM =512\\nBATCH_SIZE =128\\nNUM_ENCODER_LAYERS =3\\nNUM_DECODER_LAYERS =3\\ntransformer =Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, ␣\\n↪EMB_SIZE,\\nNHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, ␣\\n↪FFN_HID_DIM)\\nforp intransformer .parameters():\\nifp.dim()>1:\\nnn.init.xavier_uniform_(p)\\ntransformer =transformer .to(DEVICE)\\nloss_fn =torch.nn.CrossEntropyLoss(ignore_index =PAD_IDX)\\noptimizer =torch.optim.Adam(transformer .parameters(), lr =0.0001, betas=(0.9,0.\\n↪98), eps=1e-9)\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286:\\nUserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False\\nbecause encoder_layer.self_attn.batch_first was not True(use batch_first for\\n5', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 4}), Document(page_content='better inference performance)\\nwarnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is\\nFalse because {why_not_sparsity_fast_path}\")\\n2 Collation\\nwe define our collate function that converts a batch of raw strings into batch tensors that can be\\nfed directly into our model.\\n[11]: from torch .nn .utils .rnn importpad_sequence\\n# helper function to club together sequential operations\\ndefsequential_transforms (*transforms):\\ndeffunc(txt_input):\\nfortransform intransforms:\\ntxt_input =transform(txt_input)\\nreturntxt_input\\nreturnfunc\\n# function to add BOS/EOS and create tensor for input sequence indices\\ndeftensor_transform (token_ids: List[ int]):\\nreturntorch.cat((torch .tensor([BOS_IDX]),\\ntorch.tensor(token_ids),\\ntorch.tensor([EOS_IDX])))\\n# ``src`` and ``tgt`` language text transforms to convert raw strings into ␣\\n↪tensors indices\\ntext_transform ={}\\nforln in[SRC_LANGUAGE, TGT_LANGUAGE]:\\ntext_transform[ln] =sequential_transforms(token_transform[ln], ␣\\n↪#Tokenization\\nvocab_transform[ln], ␣\\n↪#Numericalization\\ntensor_transform) # Add BOS/EOS ␣\\n↪and create tensor\\n# function to collate data samples into batch tensors\\ndefcollate_fn (batch):\\nsrc_batch, tgt_batch =[], []\\nforsrc_sample, tgt_sample inbatch:\\nsrc_batch .append(text_transform[SRC_LANGUAGE](src_sample .rstrip(\"\\\\n\")))\\ntgt_batch .append(text_transform[TGT_LANGUAGE](tgt_sample .rstrip(\"\\\\n\")))\\nsrc_batch =pad_sequence(src_batch, padding_value =PAD_IDX)\\ntgt_batch =pad_sequence(tgt_batch, padding_value =PAD_IDX)\\nreturnsrc_batch, tgt_batch\\n6', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 5}), Document(page_content=\"W e now define training and evaluation loop that will be called for each epoch.\\n[12]: from torch .utils .data importDataLoader\\ndeftrain_epoch (model, optimizer):\\nmodel.train()\\nlosses=0\\ntrain_iter =Multi30k(split ='train', language_pair =(SRC_LANGUAGE, ␣\\n↪TGT_LANGUAGE))\\ntrain_dataloader =DataLoader(train_iter, batch_size =BATCH_SIZE, ␣\\n↪collate_fn =collate_fn)\\nforsrc, tgt intrain_dataloader:\\nsrc=src.to(DEVICE)\\ntgt=tgt.to(DEVICE)\\ntgt_input =tgt[:-1, :]\\nsrc_mask, tgt_mask, src_padding_mask, tgt_padding_mask =␣\\n↪create_mask(src, tgt_input)\\nlogits=model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, ␣\\n↪tgt_padding_mask, src_padding_mask)\\noptimizer .zero_grad()\\ntgt_out =tgt[1:, :]\\nloss=loss_fn(logits .reshape( -1, logits .shape[-1]), tgt_out .\\n↪reshape( -1))\\nloss.backward()\\noptimizer .step()\\nlosses+=loss.item()\\nreturnlosses/len(list(train_dataloader))\\ndefevaluate (model):\\nmodel.eval()\\nlosses=0\\nval_iter =Multi30k(split ='valid', language_pair =(SRC_LANGUAGE, ␣\\n↪TGT_LANGUAGE))\\nval_dataloader =DataLoader(val_iter, batch_size =BATCH_SIZE, ␣\\n↪collate_fn =collate_fn)\\nforsrc, tgt inval_dataloader:\\n7\", metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 6}), Document(page_content='src=src.to(DEVICE)\\ntgt=tgt.to(DEVICE)\\ntgt_input =tgt[:-1, :]\\nsrc_mask, tgt_mask, src_padding_mask, tgt_padding_mask =␣\\n↪create_mask(src, tgt_input)\\nlogits=model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, ␣\\n↪tgt_padding_mask, src_padding_mask)\\ntgt_out =tgt[1:, :]\\nloss=loss_fn(logits .reshape( -1, logits .shape[-1]), tgt_out .\\n↪reshape( -1))\\nlosses+=loss.item()\\nreturnlosses/len(list(val_dataloader))\\n[13]: from timeit importdefault_timer astimer\\nNUM_EPOCHS =25\\nforepoch inrange(1, NUM_EPOCHS +1):\\nstart_time =timer()\\ntrain_loss =train_epoch(transformer, optimizer)\\nend_time =timer()\\nval_loss =evaluate(transformer)\\nprint((f\"Epoch: {epoch }, Train loss: {train_loss :.3f }, Val loss: {val_loss :.\\n↪3f },\"f\"Epoch time = {(end_time -start_time) :.3f }s\"))\\n# function to generate output sequence using greedy algorithm\\ndefgreedy_decode (model, src, src_mask, max_len, start_symbol):\\nsrc=src.to(DEVICE)\\nsrc_mask =src_mask .to(DEVICE)\\nmemory=model.encode(src, src_mask)\\nys=torch.ones(1,1).fill_(start_symbol) .type(torch .long).to(DEVICE)\\nfori inrange(max_len -1):\\nmemory=memory.to(DEVICE)\\ntgt_mask =(generate_square_subsequent_mask(ys .size(0))\\n.type(torch .bool)).to(DEVICE)\\nout=model.decode(ys, memory, tgt_mask)\\nout=out.transpose( 0,1)\\nprob=model.generator(out[:, -1])\\n_, next_word =torch.max(prob, dim =1)\\nnext_word =next_word .item()\\n8', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 7}), Document(page_content='ys=torch.cat([ys,\\ntorch.ones(1,1).type_as(src .data).fill_(next_word)], ␣\\n↪dim=0)\\nifnext_word ==EOS_IDX:\\nbreak\\nreturnys\\n# actual function to translate input sentence into target language\\ndeftranslate (model: torch .nn.Module, src_sentence: str):\\nmodel.eval()\\nsrc=text_transform[SRC_LANGUAGE](src_sentence) .view(-1,1)\\nnum_tokens =src.shape[0]\\nsrc_mask =(torch.zeros(num_tokens, num_tokens)) .type(torch .bool)\\ntgt_tokens =greedy_decode(\\nmodel, src, src_mask, max_len =num_tokens +5, start_symbol =BOS_IDX) .\\n↪flatten()\\nreturn\"\".join(vocab_transform[TGT_LANGUAGE] .lookup_tokens( list(tgt_tokens .\\n↪cpu().numpy()))) .replace( \"<bos>\",\"\").replace( \"<eos>\",\"\")\\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5109:\\nUserWarning: Support for mismatched key_padding_mask and attn_mask is\\ndeprecated. Use same type for both instead.\\nwarnings.warn(\\n/usr/local/lib/python3.10/dist-\\npackages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some\\nchild DataPipes are not exhausted when __iter__ is called. We are resetting the\\nbuffer and each child DataPipe will read from the start again.\\nwarnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called.\\nWe are resetting \"\\nEpoch: 1, Train loss: 5.344, Val loss: 4.103, Epoch time = 114.758s\\nEpoch: 2, Train loss: 3.759, Val loss: 3.310, Epoch time = 113.627s\\nEpoch: 3, Train loss: 3.159, Val loss: 2.891, Epoch time = 114.837s\\nEpoch: 4, Train loss: 2.768, Val loss: 2.643, Epoch time = 115.061s\\nEpoch: 5, Train loss: 2.479, Val loss: 2.444, Epoch time = 115.090s\\nEpoch: 6, Train loss: 2.252, Val loss: 2.311, Epoch time = 116.286s\\nEpoch: 7, Train loss: 2.062, Val loss: 2.198, Epoch time = 115.066s\\nEpoch: 8, Train loss: 1.899, Val loss: 2.110, Epoch time = 114.630s\\nEpoch: 9, Train loss: 1.756, Val loss: 2.062, Epoch time = 115.050s\\nEpoch: 10, Train loss: 1.636, Val loss: 2.018, Epoch time = 115.461s\\nEpoch: 11, Train loss: 1.524, Val loss: 1.974, Epoch time = 115.160s\\nEpoch: 12, Train loss: 1.423, Val loss: 1.956, Epoch time = 114.551s\\nEpoch: 13, Train loss: 1.333, Val loss: 1.941, Epoch time = 116.141s\\nEpoch: 14, Train loss: 1.253, Val loss: 1.924, Epoch time = 117.210s\\nEpoch: 15, Train loss: 1.177, Val loss: 1.905, Epoch time = 117.061s\\nEpoch: 16, Train loss: 1.105, Val loss: 1.901, Epoch time = 114.374s\\nEpoch: 17, Train loss: 1.036, Val loss: 1.908, Epoch time = 117.558s\\n9', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 8}), Document(page_content='Epoch: 18, Train loss: 0.972, Val loss: 1.905, Epoch time = 117.196s\\nEpoch: 19, Train loss: 0.916, Val loss: 1.924, Epoch time = 116.272s\\nEpoch: 20, Train loss: 0.865, Val loss: 1.937, Epoch time = 115.530s\\nEpoch: 21, Train loss: 0.815, Val loss: 1.948, Epoch time = 114.853s\\nEpoch: 22, Train loss: 0.768, Val loss: 1.959, Epoch time = 117.046s\\nEpoch: 23, Train loss: 0.723, Val loss: 1.973, Epoch time = 116.390s\\nEpoch: 24, Train loss: 0.681, Val loss: 1.963, Epoch time = 115.145s\\nEpoch: 25, Train loss: 0.639, Val loss: 1.974, Epoch time = 116.800s\\n[26]:print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu. \"))\\nA group of people standing in front an igloo .\\n[15]:print(translate(transformer, \"Das Auto steht vor dem Haus. \"))\\nThe car is standing in front of the house .\\n[16]:print(translate(transformer, \"Die Katze liegt auf dem Sofa. \"))\\nThe cat is laying on the couch .\\nStep3. Insert novel sentences into your English-to-German model. T ake the output\\nand feed it to the original German-to-English model. Observe and report qualitatively\\non the results.\\n[20]:print(translate(transformer, \"Ein Tischler fängt einen Wettbewerb für eine ␣\\n↪Aufgabe. \"))\\nThere is a male athlete catching an open task for an task .\\n[21]:print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu . ␣\\n↪\"))\\nA group of people standing in front an igloo .\\n[22]:print(translate(transformer, \"Eine Gruppe von Personen steht vor einem Labor.\\n↪\"))\\nA group of people stand in front of a lab .\\n[23]:print(translate(transformer, \"Der freien Himmel scheint hell erleuchteten ␣\\n↪Himmel auf den klaren Himmel. \"))\\nA professional sky - deep lit sky in the clear blue sky .\\n[24]:print(translate(transformer, \"Kinder spielen im Park . \"))\\nThere are kids playing in the park .\\n[29]: import nltk\\nfrom nltk .translate .bleu_score importsentence_bleu\\n10', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 9}), Document(page_content='# Data initialization\\ndata=[\\n(\"A group of people stand in front of an igloo. \",\"A group of people ␣\\n↪standing in front an igloo. \"),\\n(\"The car is in front of the house \",\"The car is standing in front of the ␣\\n↪house.\"),\\n(\"The cat is lying on the sofa. \",\"The cat is laying on the couch. \"),\\n(\"A carpenter starts a competition for a task. \",\"There is a male athlete ␣\\n↪catching an open task for an task. \"),\\n(\"A group of people stand in front of a laboratory. \",\"A group of people ␣\\n↪stand in front of a lab. \"),\\n(\"The open sky shines brightly lit sky on the clear sky. \",\"A professional ␣\\n↪sky - deep lit sky in the clear blue sky. \"),\\n(\"Children play in the park. \",\"There are kids playing in the park. \")\\n]\\n# Function to calculate BLEU scores\\ndefcalculate_bleu (data):\\nscores=[]\\nforactual, translated indata:\\nreference =[actual.split()]\\ncandidate =translated .split()\\nscore=round(sentence_bleu(reference, candidate), 2)\\nscores.append(score)\\nprint(f\"Actual: {actual }\\\\nTranslated: {translated }\\\\nBLEU score: ␣\\n↪{score }\\\\n\")\\nreturnscores\\n# Calculate and print BLEU scores\\nbleu_scores =calculate_bleu(data)\\nActual: A group of people stand in front of an igloo.\\nTranslated: A group of people standing in front an igloo.\\nBLEU score: 0.36\\nActual: The car is in front of the house\\nTranslated: The car is standing in front of the house.\\nBLEU score: 0.43\\nActual: The cat is lying on the sofa.\\nTranslated: The cat is laying on the couch.\\nBLEU score: 0.0\\nActual: A carpenter starts a competition for a task.\\nTranslated: There is a male athlete catching an open task for an task.\\nBLEU score: 0.0\\n11', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 10}), Document(page_content='Actual: A group of people stand in front of a laboratory.\\nTranslated: A group of people stand in front of a lab.\\nBLEU score: 0.88\\nActual: The open sky shines brightly lit sky on the clear sky.\\nTranslated: A professional sky - deep lit sky in the clear blue sky.\\nBLEU score: 0.0\\nActual: Children play in the park.\\nTranslated: There are kids playing in the park.\\nBLEU score: 0.0\\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552:\\nUserWarning:\\nThe hypothesis contains 0 counts of 4-gram overlaps.\\nTherefore the BLEU score evaluates to 0, independently of\\nhow many N-gram overlaps of lower order it contains.\\nConsider using lower n-gram order or use SmoothingFunction()\\nwarnings.warn(_msg)\\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552:\\nUserWarning:\\nThe hypothesis contains 0 counts of 2-gram overlaps.\\nTherefore the BLEU score evaluates to 0, independently of\\nhow many N-gram overlaps of lower order it contains.\\nConsider using lower n-gram order or use SmoothingFunction()\\nwarnings.warn(_msg)\\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552:\\nUserWarning:\\nThe hypothesis contains 0 counts of 3-gram overlaps.\\nTherefore the BLEU score evaluates to 0, independently of\\nhow many N-gram overlaps of lower order it contains.\\nConsider using lower n-gram order or use SmoothingFunction()\\nwarnings.warn(_msg)\\nSNO German Actual English T ranslated English\\n1 Eine Gruppe von Menschen\\nsteht vor einem Iglu.A group of people stand in\\nfront of an igloo.A group of people standing in\\nfront an igloo .\\n2 Das Auto steht vor dem\\nHausThe car is in front of the\\nhouseThe car is standing in front of\\nthe house .\\n3 Die Katze liegt auf dem\\nSofa.The cat is lying on the sofa. The cat is laying on the couch .\\n4 Ein Tischler fängt einen\\nW ettbewerb für eine\\nAufgabe.A carpenter starts a\\ncompetition for a task.There is a male athlete\\ncatching an open task for an\\ntask .\\n5 Eine Gruppe von Personen\\nsteht vor einem Labor.A group of people stand in\\nfront of a laboratory .A group of people stand in\\nfront of a lab .\\n12', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 11}), Document(page_content='SNO German Actual English T ranslated English\\n6 Der freien Himmel scheint\\nhell erleuchteten Himmel\\nauf den klaren Himmel.The open sky shines\\nbrightly lit sky on the clear\\nsky .A professional sky - deep lit\\nsky in the clear blue sky .\\n7 Kinder spielen im Park . Children play in the park. There are kids playing in the\\npark .\\nThe BLEU scores suggest that the translation model performs poorly on some sentences (e.g.,\\nsentences 3, 4, 6, and 7) while performing relatively better on others (e.g., sentences 1, 2, and 5).\\n[17]: # Save the above model\\ntorch.save(transformer .state_dict(), \"modelGE.pt \")\\n[ ]: # Load the saved model\\ntransformer =Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, ␣\\n↪EMB_SIZE,\\nNHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, ␣\\n↪FFN_HID_DIM)\\ntransformer .load_state_dict(torch .load(\"modelGE.pt \"))\\nW e can improve the model performance by: 1. T raining the model on a larger and more di-\\nverse dataset, which can help it learn a wider range of language patterns, idiomatic expressions,\\nand domain-specific terminology . 2. Augmenting the training data with techniques like back-\\ntranslation, where sentences are translated back and forth between the source and target languages,\\ncan help expose the model to more diverse language patterns and improve its robustness 3. Apply-\\ning regularization techniques such as dropout, weight decay , or layer normalization during training\\ncan prevent overfitting and improve the generalization ability of the model.\\n[ ]:\\n13', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 12})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "#load pdf files\n",
    "loader = PyPDFLoader('AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6m662-pzyNX",
    "outputId": "c7c0cee6-379a-48d1-d488-533982aab8e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AnbuV alluvan_Devadasan_Assignment9_T ransformer_German_to_English\\nMay 8, 2024\\nStep1. Run the demo and train a model on the original German-to-English training\\nset.\\n[1]:%matplotlib inline\\n1 Data Sourcing and Processing\\nW e will use Multi30k dataset from torchtext library that yields a pair of source-target raw sentences.\\nT o access torchtext datasets, please install torchdata following instructions at https://github.\\ncom/pytorch/data .\\n[2]: from torchtext .data .utils importget_tokenizer\\nfrom torchtext .vocab importbuild_vocab_from_iterator\\nfrom torchtext .datasets importmulti30k, Multi30k\\nfrom typing importIterable, List\\nmulti30k .URL[\"train\"]=\"https://raw.githubusercontent.com/neychev/\\n↪small_DL_repo/master/datasets/Multi30k/training.tar.gz \"\\nmulti30k .URL[\"valid\"]=\"https://raw.githubusercontent.com/neychev/\\n↪small_DL_repo/master/datasets/Multi30k/validation.tar.gz \"\\nSRC_LANGUAGE =\\'de\\'\\nTGT_LANGUAGE =\\'en\\'\\n# Place-holders\\ntoken_transform ={}\\nvocab_transform ={}\\n[3]:!pip install portalocker> =2.0.0\\n[4]: import torchdata\\n[5]: import portalocker\\n[6]: # !python -m spacy download en_core_web_sm\\n# !python -m spacy download de_core_news_sm\\n1', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 0}),\n",
       " Document(page_content='[7]:token_transform[SRC_LANGUAGE] =get_tokenizer( \\'spacy\\',␣\\n↪language =\\'de_core_news_sm \\')\\ntoken_transform[TGT_LANGUAGE] =get_tokenizer( \\'spacy\\',␣\\n↪language =\\'en_core_web_sm \\')\\n# helper function to yield list of tokens\\ndefyield_tokens (data_iter: Iterable, language: str)->List[str]:\\nlanguage_index ={SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\\nfordata_sample indata_iter:\\nyieldtoken_transform[language](data_sample[language_index[language]])\\n# Define special symbols and indices\\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX =0,1,2,3\\n# Make sure the tokens are in order of their indices to properly insert them in ␣\\n↪vocab\\nspecial_symbols =[\\'<unk>\\',\\'<pad>\\',\\'<bos>\\',\\'<eos>\\']\\n\"\"\"\\nPurpose: Defines special tokens used in the vocabulary for machine learning ␣\\n↪tasks with text data.\\nSpecial Tokens:\\n<unk>: \"Unknown\" token (represents words not in the vocabulary)\\n<pad>: Padding token (to make sequences the same length)\\n<bos>: \"Beginning of Sequence\"\\n<eos>: \"End of Sequence\"\\n\"\"\"\\nforln in[SRC_LANGUAGE, TGT_LANGUAGE]:\\n# Training data Iterator\\ntrain_iter =Multi30k(split =\\'train\\', language_pair =(SRC_LANGUAGE, ␣\\n↪TGT_LANGUAGE))\\n# Create torchtext\\'s Vocab object\\nvocab_transform[ln] =build_vocab_from_iterator(yield_tokens(train_iter, ␣\\n↪ln),\\nmin_freq =1,\\nspecials =special_symbols,\\nspecial_first =True)\\n# Set ``UNK_IDX`` as the default index. This index is returned when the token ␣\\n↪is not found.\\n# If not set, it throws ``RuntimeError`` when the queried token is not found in ␣\\n↪the Vocabulary.\\nforln in[SRC_LANGUAGE, TGT_LANGUAGE]:\\nvocab_transform[ln] .set_default_index(UNK_IDX)\\n2', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 1}),\n",
       " Document(page_content=\"[8]: from torch importTensor\\nimport torch\\nimport torch .nn as nn\\nfrom torch .nn importTransformer\\nimport math\\nDEVICE=torch.device('cuda' iftorch.cuda.is_available() else'cpu')\\n# helper Module that adds positional encoding to the token embedding to ␣\\n↪introduce a notion of word order.\\nclass PositionalEncoding (nn.Module):\\ndef__init__ (self,\\nemb_size: int,\\ndropout: float,\\nmaxlen: int=5000):\\nsuper(PositionalEncoding, self).__init__ ()\\nden=torch.exp(-torch.arange(0, emb_size, 2)*math.log(10000)/␣\\n↪emb_size)\\npos=torch.arange(0, maxlen) .reshape(maxlen, 1)\\npos_embedding =torch.zeros((maxlen, emb_size))\\npos_embedding[:, 0::2]=torch.sin(pos *den)\\npos_embedding[:, 1::2]=torch.cos(pos *den)\\npos_embedding =pos_embedding .unsqueeze( -2)\\nself.dropout =nn.Dropout(dropout)\\nself.register_buffer( 'pos_embedding ', pos_embedding)\\ndefforward(self, token_embedding: Tensor):\\nreturnself.dropout(token_embedding +self.pos_embedding[:\\n↪token_embedding .size(0), :])\\n# helper Module to convert tensor of input indices into corresponding tensor of ␣\\n↪token embeddings\\nclass TokenEmbedding (nn.Module):\\ndef__init__ (self, vocab_size: int, emb_size):\\nsuper(TokenEmbedding, self).__init__ ()\\nself.embedding =nn.Embedding(vocab_size, emb_size)\\nself.emb_size =emb_size\\ndefforward(self, tokens: Tensor):\\nreturnself.embedding(tokens .long()) *math.sqrt(self.emb_size)\\n# Seq2Seq Network\\nclass Seq2SeqTransformer (nn.Module):\\ndef__init__ (self,\\nnum_encoder_layers: int,\\nnum_decoder_layers: int,\\nemb_size: int,\\n3\", metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 2}),\n",
       " Document(page_content=\"nhead:int,\\nsrc_vocab_size: int,\\ntgt_vocab_size: int,\\ndim_feedforward: int=512,\\ndropout: float=0.1):\\nsuper(Seq2SeqTransformer, self).__init__ ()\\nself.transformer =Transformer(d_model =emb_size,\\nnhead=nhead,\\nnum_encoder_layers =num_encoder_layers,\\nnum_decoder_layers =num_decoder_layers,\\ndim_feedforward =dim_feedforward,\\ndropout=dropout)\\nself.generator =nn.Linear(emb_size, tgt_vocab_size)\\nself.src_tok_emb =TokenEmbedding(src_vocab_size, emb_size)\\nself.tgt_tok_emb =TokenEmbedding(tgt_vocab_size, emb_size)\\nself.positional_encoding =PositionalEncoding(\\nemb_size, dropout =dropout)\\ndefforward(self,\\nsrc: Tensor,\\ntrg: Tensor,\\nsrc_mask: Tensor,\\ntgt_mask: Tensor,\\nsrc_padding_mask: Tensor,\\ntgt_padding_mask: Tensor,\\nmemory_key_padding_mask: Tensor):\\nsrc_emb =self.positional_encoding( self.src_tok_emb(src))\\ntgt_emb =self.positional_encoding( self.tgt_tok_emb(trg))\\nouts=self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\\nsrc_padding_mask, tgt_padding_mask, ␣\\n↪memory_key_padding_mask)\\nreturnself.generator(outs)\\ndefencode(self, src: Tensor, src_mask: Tensor):\\nreturnself.transformer .encoder( self.positional_encoding(\\nself.src_tok_emb(src)), src_mask)\\ndefdecode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\\nreturnself.transformer .decoder( self.positional_encoding(\\nself.tgt_tok_emb(tgt)), memory,\\ntgt_mask)\\n[9]: defgenerate_square_subsequent_mask (sz):\\nmask=(torch.triu(torch .ones((sz, sz), device =DEVICE)) ==1).transpose( 0,␣\\n↪1)\\nmask=mask.float().masked_fill(mask ==0,float('-inf')).masked_fill(mask ␣\\n↪==1,float(0.0))\\n4\", metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 3}),\n",
       " Document(page_content='returnmask\\ndefcreate_mask (src, tgt):\\nsrc_seq_len =src.shape[0]\\ntgt_seq_len =tgt.shape[0]\\ntgt_mask =generate_square_subsequent_mask(tgt_seq_len)\\nsrc_mask =torch.zeros((src_seq_len, src_seq_len),device =DEVICE).type(torch .\\n↪bool)\\nsrc_padding_mask =(src==PAD_IDX) .transpose( 0,1)\\ntgt_padding_mask =(tgt==PAD_IDX) .transpose( 0,1)\\nreturnsrc_mask, tgt_mask, src_padding_mask, tgt_padding_mask\\nW e now define the parameters of our model and instantiate the same. Below, we also define our\\nloss function which is the cross-entropy loss and the optimizer used for training.\\n[10]:torch.manual_seed( 0)\\nSRC_VOCAB_SIZE =len(vocab_transform[SRC_LANGUAGE])\\nTGT_VOCAB_SIZE =len(vocab_transform[TGT_LANGUAGE])\\nEMB_SIZE =512\\nNHEAD=8\\nFFN_HID_DIM =512\\nBATCH_SIZE =128\\nNUM_ENCODER_LAYERS =3\\nNUM_DECODER_LAYERS =3\\ntransformer =Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, ␣\\n↪EMB_SIZE,\\nNHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, ␣\\n↪FFN_HID_DIM)\\nforp intransformer .parameters():\\nifp.dim()>1:\\nnn.init.xavier_uniform_(p)\\ntransformer =transformer .to(DEVICE)\\nloss_fn =torch.nn.CrossEntropyLoss(ignore_index =PAD_IDX)\\noptimizer =torch.optim.Adam(transformer .parameters(), lr =0.0001, betas=(0.9,0.\\n↪98), eps=1e-9)\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286:\\nUserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False\\nbecause encoder_layer.self_attn.batch_first was not True(use batch_first for\\n5', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 4}),\n",
       " Document(page_content='better inference performance)\\nwarnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is\\nFalse because {why_not_sparsity_fast_path}\")\\n2 Collation\\nwe define our collate function that converts a batch of raw strings into batch tensors that can be\\nfed directly into our model.\\n[11]: from torch .nn .utils .rnn importpad_sequence\\n# helper function to club together sequential operations\\ndefsequential_transforms (*transforms):\\ndeffunc(txt_input):\\nfortransform intransforms:\\ntxt_input =transform(txt_input)\\nreturntxt_input\\nreturnfunc\\n# function to add BOS/EOS and create tensor for input sequence indices\\ndeftensor_transform (token_ids: List[ int]):\\nreturntorch.cat((torch .tensor([BOS_IDX]),\\ntorch.tensor(token_ids),\\ntorch.tensor([EOS_IDX])))\\n# ``src`` and ``tgt`` language text transforms to convert raw strings into ␣\\n↪tensors indices\\ntext_transform ={}\\nforln in[SRC_LANGUAGE, TGT_LANGUAGE]:\\ntext_transform[ln] =sequential_transforms(token_transform[ln], ␣\\n↪#Tokenization\\nvocab_transform[ln], ␣\\n↪#Numericalization\\ntensor_transform) # Add BOS/EOS ␣\\n↪and create tensor\\n# function to collate data samples into batch tensors\\ndefcollate_fn (batch):\\nsrc_batch, tgt_batch =[], []\\nforsrc_sample, tgt_sample inbatch:\\nsrc_batch .append(text_transform[SRC_LANGUAGE](src_sample .rstrip(\"\\\\n\")))\\ntgt_batch .append(text_transform[TGT_LANGUAGE](tgt_sample .rstrip(\"\\\\n\")))\\nsrc_batch =pad_sequence(src_batch, padding_value =PAD_IDX)\\ntgt_batch =pad_sequence(tgt_batch, padding_value =PAD_IDX)\\nreturnsrc_batch, tgt_batch\\n6', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 5}),\n",
       " Document(page_content=\"W e now define training and evaluation loop that will be called for each epoch.\\n[12]: from torch .utils .data importDataLoader\\ndeftrain_epoch (model, optimizer):\\nmodel.train()\\nlosses=0\\ntrain_iter =Multi30k(split ='train', language_pair =(SRC_LANGUAGE, ␣\\n↪TGT_LANGUAGE))\\ntrain_dataloader =DataLoader(train_iter, batch_size =BATCH_SIZE, ␣\\n↪collate_fn =collate_fn)\\nforsrc, tgt intrain_dataloader:\\nsrc=src.to(DEVICE)\\ntgt=tgt.to(DEVICE)\\ntgt_input =tgt[:-1, :]\\nsrc_mask, tgt_mask, src_padding_mask, tgt_padding_mask =␣\\n↪create_mask(src, tgt_input)\\nlogits=model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, ␣\\n↪tgt_padding_mask, src_padding_mask)\\noptimizer .zero_grad()\\ntgt_out =tgt[1:, :]\\nloss=loss_fn(logits .reshape( -1, logits .shape[-1]), tgt_out .\\n↪reshape( -1))\\nloss.backward()\\noptimizer .step()\\nlosses+=loss.item()\\nreturnlosses/len(list(train_dataloader))\\ndefevaluate (model):\\nmodel.eval()\\nlosses=0\\nval_iter =Multi30k(split ='valid', language_pair =(SRC_LANGUAGE, ␣\\n↪TGT_LANGUAGE))\\nval_dataloader =DataLoader(val_iter, batch_size =BATCH_SIZE, ␣\\n↪collate_fn =collate_fn)\\nforsrc, tgt inval_dataloader:\\n7\", metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 6}),\n",
       " Document(page_content='src=src.to(DEVICE)\\ntgt=tgt.to(DEVICE)\\ntgt_input =tgt[:-1, :]\\nsrc_mask, tgt_mask, src_padding_mask, tgt_padding_mask =␣\\n↪create_mask(src, tgt_input)\\nlogits=model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, ␣\\n↪tgt_padding_mask, src_padding_mask)\\ntgt_out =tgt[1:, :]\\nloss=loss_fn(logits .reshape( -1, logits .shape[-1]), tgt_out .\\n↪reshape( -1))\\nlosses+=loss.item()\\nreturnlosses/len(list(val_dataloader))\\n[13]: from timeit importdefault_timer astimer\\nNUM_EPOCHS =25\\nforepoch inrange(1, NUM_EPOCHS +1):\\nstart_time =timer()\\ntrain_loss =train_epoch(transformer, optimizer)\\nend_time =timer()\\nval_loss =evaluate(transformer)\\nprint((f\"Epoch: {epoch }, Train loss: {train_loss :.3f }, Val loss: {val_loss :.\\n↪3f },\"f\"Epoch time = {(end_time -start_time) :.3f }s\"))\\n# function to generate output sequence using greedy algorithm\\ndefgreedy_decode (model, src, src_mask, max_len, start_symbol):\\nsrc=src.to(DEVICE)\\nsrc_mask =src_mask .to(DEVICE)\\nmemory=model.encode(src, src_mask)\\nys=torch.ones(1,1).fill_(start_symbol) .type(torch .long).to(DEVICE)\\nfori inrange(max_len -1):\\nmemory=memory.to(DEVICE)\\ntgt_mask =(generate_square_subsequent_mask(ys .size(0))\\n.type(torch .bool)).to(DEVICE)\\nout=model.decode(ys, memory, tgt_mask)\\nout=out.transpose( 0,1)\\nprob=model.generator(out[:, -1])\\n_, next_word =torch.max(prob, dim =1)\\nnext_word =next_word .item()\\n8', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 7}),\n",
       " Document(page_content='ys=torch.cat([ys,\\ntorch.ones(1,1).type_as(src .data).fill_(next_word)], ␣\\n↪dim=0)\\nifnext_word ==EOS_IDX:\\nbreak\\nreturnys\\n# actual function to translate input sentence into target language\\ndeftranslate (model: torch .nn.Module, src_sentence: str):\\nmodel.eval()\\nsrc=text_transform[SRC_LANGUAGE](src_sentence) .view(-1,1)\\nnum_tokens =src.shape[0]\\nsrc_mask =(torch.zeros(num_tokens, num_tokens)) .type(torch .bool)\\ntgt_tokens =greedy_decode(\\nmodel, src, src_mask, max_len =num_tokens +5, start_symbol =BOS_IDX) .\\n↪flatten()\\nreturn\"\".join(vocab_transform[TGT_LANGUAGE] .lookup_tokens( list(tgt_tokens .\\n↪cpu().numpy()))) .replace( \"<bos>\",\"\").replace( \"<eos>\",\"\")\\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5109:\\nUserWarning: Support for mismatched key_padding_mask and attn_mask is\\ndeprecated. Use same type for both instead.\\nwarnings.warn(\\n/usr/local/lib/python3.10/dist-\\npackages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some\\nchild DataPipes are not exhausted when __iter__ is called. We are resetting the\\nbuffer and each child DataPipe will read from the start again.\\nwarnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called.\\nWe are resetting \"\\nEpoch: 1, Train loss: 5.344, Val loss: 4.103, Epoch time = 114.758s\\nEpoch: 2, Train loss: 3.759, Val loss: 3.310, Epoch time = 113.627s\\nEpoch: 3, Train loss: 3.159, Val loss: 2.891, Epoch time = 114.837s\\nEpoch: 4, Train loss: 2.768, Val loss: 2.643, Epoch time = 115.061s\\nEpoch: 5, Train loss: 2.479, Val loss: 2.444, Epoch time = 115.090s\\nEpoch: 6, Train loss: 2.252, Val loss: 2.311, Epoch time = 116.286s\\nEpoch: 7, Train loss: 2.062, Val loss: 2.198, Epoch time = 115.066s\\nEpoch: 8, Train loss: 1.899, Val loss: 2.110, Epoch time = 114.630s\\nEpoch: 9, Train loss: 1.756, Val loss: 2.062, Epoch time = 115.050s\\nEpoch: 10, Train loss: 1.636, Val loss: 2.018, Epoch time = 115.461s\\nEpoch: 11, Train loss: 1.524, Val loss: 1.974, Epoch time = 115.160s\\nEpoch: 12, Train loss: 1.423, Val loss: 1.956, Epoch time = 114.551s\\nEpoch: 13, Train loss: 1.333, Val loss: 1.941, Epoch time = 116.141s\\nEpoch: 14, Train loss: 1.253, Val loss: 1.924, Epoch time = 117.210s\\nEpoch: 15, Train loss: 1.177, Val loss: 1.905, Epoch time = 117.061s\\nEpoch: 16, Train loss: 1.105, Val loss: 1.901, Epoch time = 114.374s\\nEpoch: 17, Train loss: 1.036, Val loss: 1.908, Epoch time = 117.558s\\n9', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 8}),\n",
       " Document(page_content='Epoch: 18, Train loss: 0.972, Val loss: 1.905, Epoch time = 117.196s\\nEpoch: 19, Train loss: 0.916, Val loss: 1.924, Epoch time = 116.272s\\nEpoch: 20, Train loss: 0.865, Val loss: 1.937, Epoch time = 115.530s\\nEpoch: 21, Train loss: 0.815, Val loss: 1.948, Epoch time = 114.853s\\nEpoch: 22, Train loss: 0.768, Val loss: 1.959, Epoch time = 117.046s\\nEpoch: 23, Train loss: 0.723, Val loss: 1.973, Epoch time = 116.390s\\nEpoch: 24, Train loss: 0.681, Val loss: 1.963, Epoch time = 115.145s\\nEpoch: 25, Train loss: 0.639, Val loss: 1.974, Epoch time = 116.800s\\n[26]:print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu. \"))\\nA group of people standing in front an igloo .\\n[15]:print(translate(transformer, \"Das Auto steht vor dem Haus. \"))\\nThe car is standing in front of the house .\\n[16]:print(translate(transformer, \"Die Katze liegt auf dem Sofa. \"))\\nThe cat is laying on the couch .\\nStep3. Insert novel sentences into your English-to-German model. T ake the output\\nand feed it to the original German-to-English model. Observe and report qualitatively\\non the results.\\n[20]:print(translate(transformer, \"Ein Tischler fängt einen Wettbewerb für eine ␣\\n↪Aufgabe. \"))\\nThere is a male athlete catching an open task for an task .\\n[21]:print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu . ␣\\n↪\"))\\nA group of people standing in front an igloo .\\n[22]:print(translate(transformer, \"Eine Gruppe von Personen steht vor einem Labor.\\n↪\"))\\nA group of people stand in front of a lab .\\n[23]:print(translate(transformer, \"Der freien Himmel scheint hell erleuchteten ␣\\n↪Himmel auf den klaren Himmel. \"))\\nA professional sky - deep lit sky in the clear blue sky .\\n[24]:print(translate(transformer, \"Kinder spielen im Park . \"))\\nThere are kids playing in the park .\\n[29]: import nltk\\nfrom nltk .translate .bleu_score importsentence_bleu\\n10', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 9}),\n",
       " Document(page_content='# Data initialization\\ndata=[\\n(\"A group of people stand in front of an igloo. \",\"A group of people ␣\\n↪standing in front an igloo. \"),\\n(\"The car is in front of the house \",\"The car is standing in front of the ␣\\n↪house.\"),\\n(\"The cat is lying on the sofa. \",\"The cat is laying on the couch. \"),\\n(\"A carpenter starts a competition for a task. \",\"There is a male athlete ␣\\n↪catching an open task for an task. \"),\\n(\"A group of people stand in front of a laboratory. \",\"A group of people ␣\\n↪stand in front of a lab. \"),\\n(\"The open sky shines brightly lit sky on the clear sky. \",\"A professional ␣\\n↪sky - deep lit sky in the clear blue sky. \"),\\n(\"Children play in the park. \",\"There are kids playing in the park. \")\\n]\\n# Function to calculate BLEU scores\\ndefcalculate_bleu (data):\\nscores=[]\\nforactual, translated indata:\\nreference =[actual.split()]\\ncandidate =translated .split()\\nscore=round(sentence_bleu(reference, candidate), 2)\\nscores.append(score)\\nprint(f\"Actual: {actual }\\\\nTranslated: {translated }\\\\nBLEU score: ␣\\n↪{score }\\\\n\")\\nreturnscores\\n# Calculate and print BLEU scores\\nbleu_scores =calculate_bleu(data)\\nActual: A group of people stand in front of an igloo.\\nTranslated: A group of people standing in front an igloo.\\nBLEU score: 0.36\\nActual: The car is in front of the house\\nTranslated: The car is standing in front of the house.\\nBLEU score: 0.43\\nActual: The cat is lying on the sofa.\\nTranslated: The cat is laying on the couch.\\nBLEU score: 0.0\\nActual: A carpenter starts a competition for a task.\\nTranslated: There is a male athlete catching an open task for an task.\\nBLEU score: 0.0\\n11', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 10}),\n",
       " Document(page_content='Actual: A group of people stand in front of a laboratory.\\nTranslated: A group of people stand in front of a lab.\\nBLEU score: 0.88\\nActual: The open sky shines brightly lit sky on the clear sky.\\nTranslated: A professional sky - deep lit sky in the clear blue sky.\\nBLEU score: 0.0\\nActual: Children play in the park.\\nTranslated: There are kids playing in the park.\\nBLEU score: 0.0\\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552:\\nUserWarning:\\nThe hypothesis contains 0 counts of 4-gram overlaps.\\nTherefore the BLEU score evaluates to 0, independently of\\nhow many N-gram overlaps of lower order it contains.\\nConsider using lower n-gram order or use SmoothingFunction()\\nwarnings.warn(_msg)\\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552:\\nUserWarning:\\nThe hypothesis contains 0 counts of 2-gram overlaps.\\nTherefore the BLEU score evaluates to 0, independently of\\nhow many N-gram overlaps of lower order it contains.\\nConsider using lower n-gram order or use SmoothingFunction()\\nwarnings.warn(_msg)\\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552:\\nUserWarning:\\nThe hypothesis contains 0 counts of 3-gram overlaps.\\nTherefore the BLEU score evaluates to 0, independently of\\nhow many N-gram overlaps of lower order it contains.\\nConsider using lower n-gram order or use SmoothingFunction()\\nwarnings.warn(_msg)\\nSNO German Actual English T ranslated English\\n1 Eine Gruppe von Menschen\\nsteht vor einem Iglu.A group of people stand in\\nfront of an igloo.A group of people standing in\\nfront an igloo .\\n2 Das Auto steht vor dem\\nHausThe car is in front of the\\nhouseThe car is standing in front of\\nthe house .\\n3 Die Katze liegt auf dem\\nSofa.The cat is lying on the sofa. The cat is laying on the couch .\\n4 Ein Tischler fängt einen\\nW ettbewerb für eine\\nAufgabe.A carpenter starts a\\ncompetition for a task.There is a male athlete\\ncatching an open task for an\\ntask .\\n5 Eine Gruppe von Personen\\nsteht vor einem Labor.A group of people stand in\\nfront of a laboratory .A group of people stand in\\nfront of a lab .\\n12', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 11}),\n",
       " Document(page_content='SNO German Actual English T ranslated English\\n6 Der freien Himmel scheint\\nhell erleuchteten Himmel\\nauf den klaren Himmel.The open sky shines\\nbrightly lit sky on the clear\\nsky .A professional sky - deep lit\\nsky in the clear blue sky .\\n7 Kinder spielen im Park . Children play in the park. There are kids playing in the\\npark .\\nThe BLEU scores suggest that the translation model performs poorly on some sentences (e.g.,\\nsentences 3, 4, 6, and 7) while performing relatively better on others (e.g., sentences 1, 2, and 5).\\n[17]: # Save the above model\\ntorch.save(transformer .state_dict(), \"modelGE.pt \")\\n[ ]: # Load the saved model\\ntransformer =Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, ␣\\n↪EMB_SIZE,\\nNHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, ␣\\n↪FFN_HID_DIM)\\ntransformer .load_state_dict(torch .load(\"modelGE.pt \"))\\nW e can improve the model performance by: 1. T raining the model on a larger and more di-\\nverse dataset, which can help it learn a wider range of language patterns, idiomatic expressions,\\nand domain-specific terminology . 2. Augmenting the training data with techniques like back-\\ntranslation, where sentences are translated back and forth between the source and target languages,\\ncan help expose the model to more diverse language patterns and improve its robustness 3. Apply-\\ning regularization techniques such as dropout, weight decay , or layer normalization during training\\ncan prevent overfitting and improve the generalization ability of the model.\\n[ ]:\\n13', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdORRTUoi3vp",
    "outputId": "8aef10ca-aa90-446b-c5a8-393cf7584a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split text data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=25)\n",
    "text_chunks = text_splitter.split_documents(data)\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3t1UdQztjFp6",
    "outputId": "be03ea88-6b70-4338-e96d-99b356e34ee3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='[7]:token_transform[SRC_LANGUAGE] =get_tokenizer( \\'spacy\\',␣\\n↪language =\\'de_core_news_sm \\')\\ntoken_transform[TGT_LANGUAGE] =get_tokenizer( \\'spacy\\',␣\\n↪language =\\'en_core_web_sm \\')\\n# helper function to yield list of tokens\\ndefyield_tokens (data_iter: Iterable, language: str)->List[str]:\\nlanguage_index ={SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\\nfordata_sample indata_iter:\\nyieldtoken_transform[language](data_sample[language_index[language]])\\n# Define special symbols and indices\\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX =0,1,2,3\\n# Make sure the tokens are in order of their indices to properly insert them in ␣\\n↪vocab\\nspecial_symbols =[\\'<unk>\\',\\'<pad>\\',\\'<bos>\\',\\'<eos>\\']\\n\"\"\"\\nPurpose: Defines special tokens used in the vocabulary for machine learning ␣\\n↪tasks with text data.\\nSpecial Tokens:\\n<unk>: \"Unknown\" token (represents words not in the vocabulary)\\n<pad>: Padding token (to make sequences the same length)\\n<bos>: \"Beginning of Sequence\"\\n<eos>: \"End of Sequence\"\\n\"\"\"\\nforln in[SRC_LANGUAGE, TGT_LANGUAGE]:\\n# Training data Iterator', metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the chunks\n",
    "text_chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyeGKo8-jqAf",
    "outputId": "a6dd63b0-b88e-41b3-f5dd-43ca8f356456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doi': '', 'chunk-id': '3', 'chunk': Document(page_content=\"# Training data Iterator\\ntrain_iter =Multi30k(split ='train', language_pair =(SRC_LANGUAGE, ␣\\n↪TGT_LANGUAGE))\\n# Create torchtext's Vocab object\\nvocab_transform[ln] =build_vocab_from_iterator(yield_tokens(train_iter, ␣\\n↪ln),\\nmin_freq =1,\\nspecials =special_symbols,\\nspecial_first =True)\\n# Set ``UNK_IDX`` as the default index. This index is returned when the token ␣\\n↪is not found.\\n# If not set, it throws ``RuntimeError`` when the queried token is not found in ␣\\n↪the Vocabulary.\\nforln in[SRC_LANGUAGE, TGT_LANGUAGE]:\\nvocab_transform[ln] .set_default_index(UNK_IDX)\\n2\", metadata={'source': 'AnbuValluvan_Devadasan_Assignment9_Transformer_German_to_English.pdf', 'page': 1}), 'id': '', 'title': '', 'summary': '', 'source': '', 'authors': [], 'categories': [], 'comment': '', 'journal_ref': None, 'primary_category': '', 'published': '', 'updated': '', 'references': []}\n"
     ]
    }
   ],
   "source": [
    "# reformat chunks to improve vectorization; match 'jamescalam/llama-2-arxiv-papers-chunked' format sourced from Llama 2 ArXiv papers on huggingface\n",
    "dataset = []\n",
    "\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    dataset.append({\n",
    "        'doi': '',  # you can add a DOI here if available\n",
    "        'chunk-id': str(i),\n",
    "        'chunk': chunk,\n",
    "        'id': '',  # you can add an ID here if available\n",
    "        'title': '',  # you can add a title here if available\n",
    "        'summary': '',  # you can add a summary here if available\n",
    "        'source': '',  # you can add a source here if available\n",
    "        'authors': [],  # you can add authors here if available\n",
    "        'categories': [],  # you can add categories here if available\n",
    "        'comment': '',  # you can add a comment here if available\n",
    "        'journal_ref': None,  # you can add a journal reference here if available\n",
    "        'primary_category': '',  # you can add a primary category here if available\n",
    "        'published': '',  # you can add a published date here if available\n",
    "        'updated': '',  # you can add an updated date here if available\n",
    "        'references': []  # you can add references here if available\n",
    "    })\n",
    "\n",
    "print(dataset[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo1AYqcZw4wY"
   },
   "source": [
    "#### Dataset Overview\n",
    "\n",
    "The dataset used is the Assignment on Trasnformers in DATA_255\n",
    "\n",
    "Because most **Large Language Models** (**LLMs**) only contain knowledge of the world as it was during training, they cannot answer our questions about Jorge's code without example data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq3-dxkGw4wY"
   },
   "source": [
    "### Task 4: Building the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsYU27hBw4wY"
   },
   "source": [
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our connection to Pinecone, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GxIvcXXOw4wb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize connection to Pinecone using the environment variable\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kcal_JEgw4wb"
   },
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uTLMWZPAw4wb"
   },
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSxNjSjLw4wb"
   },
   "source": [
    "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULRvhj4aw4wb",
    "outputId": "91fc9dca-c22e-4576-cf36-7e680c25089a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'llama-2-rag'\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of ada 002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jae7TDz5w4wb"
   },
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model — we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCQ-XA0Vw4wb",
    "outputId": "79ac12d7-b90f-4841-dde7-120eb2c9f224"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONTjqjWww4wb"
   },
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugECVXqDw4wb",
    "outputId": "b5b89269-5da0-4a8c-c8df-96c4024724b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f82zGPmIw4wb"
   },
   "source": [
    "From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches.\n",
    "\n",
    "**NOTE**: *ensure that chunks are strings and ensure that they are correctly assigned to metadata (do this with the .page_content method)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "147db994a0ef40e8b156ec8efc282216",
      "e6f67823ccbe4575b67ef366559ef115",
      "e2251259cba249868868886d109ae35c",
      "1ed3b66d5f8948b79d70985d2da9bba0",
      "9918148e66a640e98ba43fdc47b8f91d",
      "47036c7a921a410e84e5ab9f538e162f",
      "c90b082e6ab64e9d9febe55ef7606f4a",
      "5facec2d0cc14519aa26f493dc202160",
      "4ad55880bd4443dd8a87981dea80b4d6",
      "b3f212423af74648bb9fb9ded43a20d9",
      "14f46db244884f08b5fde46fa48fa227"
     ]
    },
    "id": "AtgH_iMuw4wb",
    "outputId": "1efca4c4-a2a6-4ce0-bbc8-a1219a5b7df9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = pd.DataFrame(dataset) # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [str(x['chunk']) for _, x in batch.iterrows()]\n",
    "\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'].page_content,\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9p8OBkyw4wc"
   },
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3QdqBijw4wc",
    "outputId": "1d500ff5-e2b8-4abb-f3e5-60003a45da60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 28}},\n",
       " 'total_vector_count': 28}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqqVoZnkw4wc"
   },
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcm-dCPCw4wc"
   },
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJPGGyXSw4wc"
   },
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4MPMJmtCw4wc",
    "outputId": "9062538f-8112-4642-a4a9-b12bd07a40ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.vectorstores.pinecone.Pinecone` was deprecated in langchain-community 0.0.18 and will be removed in 0.2.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zrlwGYUw4wc"
   },
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyShvreew4wc",
    "outputId": "2234861a-107c-4575-d08d-18e035b14d1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AnbuV alluvan_Devadasan_Assignment9_T ransformer_German_to_English\\nMay 8, 2024\\nStep1. Run the demo and train a model on the original German-to-English training\\nset.\\n[1]:%matplotlib inline\\n1 Data Sourcing and Processing\\nW e will use Multi30k dataset from torchtext library that yields a pair of source-target raw sentences.\\nT o access torchtext datasets, please install torchdata following instructions at https://github.\\ncom/pytorch/data .\\n[2]: from torchtext .data .utils importget_tokenizer\\nfrom torchtext .vocab importbuild_vocab_from_iterator\\nfrom torchtext .datasets importmulti30k, Multi30k\\nfrom typing importIterable, List\\nmulti30k .URL[\"train\"]=\"https://raw.githubusercontent.com/neychev/\\n↪small_DL_repo/master/datasets/Multi30k/training.tar.gz \"\\nmulti30k .URL[\"valid\"]=\"https://raw.githubusercontent.com/neychev/\\n↪small_DL_repo/master/datasets/Multi30k/validation.tar.gz \"\\nSRC_LANGUAGE =\\'de\\'\\nTGT_LANGUAGE =\\'en\\'\\n# Place-holders\\ntoken_transform ={}\\nvocab_transform ={}\\n[3]:!pip install portalocker> =2.0.0', metadata={'source': '', 'title': ''}),\n",
       " Document(page_content=\"[8]: from torch importTensor\\nimport torch\\nimport torch .nn as nn\\nfrom torch .nn importTransformer\\nimport math\\nDEVICE=torch.device('cuda' iftorch.cuda.is_available() else'cpu')\\n# helper Module that adds positional encoding to the token embedding to ␣\\n↪introduce a notion of word order.\\nclass PositionalEncoding (nn.Module):\\ndef__init__ (self,\\nemb_size: int,\\ndropout: float,\\nmaxlen: int=5000):\\nsuper(PositionalEncoding, self).__init__ ()\\nden=torch.exp(-torch.arange(0, emb_size, 2)*math.log(10000)/␣\\n↪emb_size)\\npos=torch.arange(0, maxlen) .reshape(maxlen, 1)\\npos_embedding =torch.zeros((maxlen, emb_size))\\npos_embedding[:, 0::2]=torch.sin(pos *den)\\npos_embedding[:, 1::2]=torch.cos(pos *den)\\npos_embedding =pos_embedding .unsqueeze( -2)\\nself.dropout =nn.Dropout(dropout)\\nself.register_buffer( 'pos_embedding ', pos_embedding)\\ndefforward(self, token_embedding: Tensor):\\nreturnself.dropout(token_embedding +self.pos_embedding[:\\n↪token_embedding .size(0), :])\", metadata={'source': '', 'title': ''}),\n",
       " Document(page_content='and domain-specific terminology . 2. Augmenting the training data with techniques like back-\\ntranslation, where sentences are translated back and forth between the source and target languages,\\ncan help expose the model to more diverse language patterns and improve its robustness 3. Apply-\\ning regularization techniques such as dropout, weight decay , or layer normalization during training\\ncan prevent overfitting and improve the generalization ability of the model.\\n[ ]:\\n13', metadata={'source': '', 'title': ''})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Did they use Pytorch or Tensorflow in the Assignment?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huzLImnKw4wc"
   },
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Rp5NBaqfw4wc"
   },
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2-7QCTew4wc"
   },
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mftcb16Cw4wc",
    "outputId": "fe5a660e-68f9-4d82-ba42-4f59c440a04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    AnbuV alluvan_Devadasan_Assignment9_T ransformer_German_to_English\n",
      "May 8, 2024\n",
      "Step1. Run the demo and train a model on the original German-to-English training\n",
      "set.\n",
      "[1]:%matplotlib inline\n",
      "1 Data Sourcing and Processing\n",
      "W e will use Multi30k dataset from torchtext library that yields a pair of source-target raw sentences.\n",
      "T o access torchtext datasets, please install torchdata following instructions at https://github.\n",
      "com/pytorch/data .\n",
      "[2]: from torchtext .data .utils importget_tokenizer\n",
      "from torchtext .vocab importbuild_vocab_from_iterator\n",
      "from torchtext .datasets importmulti30k, Multi30k\n",
      "from typing importIterable, List\n",
      "multi30k .URL[\"train\"]=\"https://raw.githubusercontent.com/neychev/\n",
      "↪small_DL_repo/master/datasets/Multi30k/training.tar.gz \"\n",
      "multi30k .URL[\"valid\"]=\"https://raw.githubusercontent.com/neychev/\n",
      "↪small_DL_repo/master/datasets/Multi30k/validation.tar.gz \"\n",
      "SRC_LANGUAGE ='de'\n",
      "TGT_LANGUAGE ='en'\n",
      "# Place-holders\n",
      "token_transform ={}\n",
      "vocab_transform ={}\n",
      "[3]:!pip install portalocker> =2.0.0\n",
      "[8]: from torch importTensor\n",
      "import torch\n",
      "import torch .nn as nn\n",
      "from torch .nn importTransformer\n",
      "import math\n",
      "DEVICE=torch.device('cuda' iftorch.cuda.is_available() else'cpu')\n",
      "# helper Module that adds positional encoding to the token embedding to ␣\n",
      "↪introduce a notion of word order.\n",
      "class PositionalEncoding (nn.Module):\n",
      "def__init__ (self,\n",
      "emb_size: int,\n",
      "dropout: float,\n",
      "maxlen: int=5000):\n",
      "super(PositionalEncoding, self).__init__ ()\n",
      "den=torch.exp(-torch.arange(0, emb_size, 2)*math.log(10000)/␣\n",
      "↪emb_size)\n",
      "pos=torch.arange(0, maxlen) .reshape(maxlen, 1)\n",
      "pos_embedding =torch.zeros((maxlen, emb_size))\n",
      "pos_embedding[:, 0::2]=torch.sin(pos *den)\n",
      "pos_embedding[:, 1::2]=torch.cos(pos *den)\n",
      "pos_embedding =pos_embedding .unsqueeze( -2)\n",
      "self.dropout =nn.Dropout(dropout)\n",
      "self.register_buffer( 'pos_embedding ', pos_embedding)\n",
      "defforward(self, token_embedding: Tensor):\n",
      "returnself.dropout(token_embedding +self.pos_embedding[:\n",
      "↪token_embedding .size(0), :])\n",
      "and domain-specific terminology . 2. Augmenting the training data with techniques like back-\n",
      "translation, where sentences are translated back and forth between the source and target languages,\n",
      "can help expose the model to more diverse language patterns and improve its robustness 3. Apply-\n",
      "ing regularization techniques such as dropout, weight decay , or layer normalization during training\n",
      "can prevent overfitting and improve the generalization ability of the model.\n",
      "[ ]:\n",
      "13\n",
      "\n",
      "    Query: Did they use Pytorch or Tensorflow in the Assignment?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3j-9JMRWw4wc",
    "outputId": "9f25f18f-218f-4fd6-9328-e6c1af934003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They used PyTorch in the assignment. This can be inferred from the code snippets provided in the context, where they imported and utilized PyTorch modules and classes such as `torch`, `torch.nn`, and `nn`. The specific lines of code indicate the use of PyTorch for building and training the model for German-to-English translation.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwWgr78fw4wc"
   },
   "source": [
    "We can continue with more questions about the Assignment. Let's try _without_ RAG first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBuJCW1ew4wc",
    "outputId": "79dc2ba5-ccca-4f57-ddbb-0d8a03f6097d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Assignment described, they used the Multi30k dataset from the torchtext library. The Multi30k dataset provides a pair of source-target raw sentences for training machine translation models from German to English. The dataset was accessed using torchtext datasets and was used to train a model for German-to-English translation.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"Which dataset did they use in the Assignment?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ts3kGD0w4wc"
   },
   "source": [
    "The chatbot is able to respond about the assignment thanks to it's conversational history stored in `messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuWbaJAIw4wc",
    "outputId": "719e1ae6-f17c-4bab-9219-2e5a91837797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the assignment \"Transformer_German_to_English\" mentioned in the context, they used a deep learning model called \"Seq2SeqTransformer.\" This model was trained and utilized for translating text from German to English.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"Which deep learning model did they use in the assignment?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZq8hZQemR15",
    "outputId": "b011c6cd-ac77-48f1-9a20-09677d065d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the code snippet that defines the `Seq2SeqTransformer` class in the context provided:\n",
      "\n",
      "```python\n",
      "# Seq2Seq Network\n",
      "class Seq2SeqTransformer(nn.Module):\n",
      "    def __init__(self,\n",
      "                 num_encoder_layers: int,\n",
      "                 num_decoder_layers: int,\n",
      "                 emb_size: int,\n",
      "                 nhead: int,\n",
      "                 src_vocab_size: int,\n",
      "                 tgt_vocab_size: int,\n",
      "                 dim_feedforward: int = 512,\n",
      "                 dropout: float = 0.1):\n",
      "        super(Seq2SeqTransformer, self).__init__()\n",
      "        self.transformer = Transformer(d_model=emb_size,\n",
      "                                       nhead=nhead,\n",
      "                                       num_encoder_layers=num_encoder_layers,\n",
      "                                       num_decoder_layers=num_decoder_layers,\n",
      "                                       dim_feedforward=dim_feedforward,\n",
      "                                       dropout=dropout)\n",
      "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
      "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
      "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
      "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
      "\n",
      "    def forward(self,\n",
      "                src: Tensor,\n",
      "                trg: Tensor,\n",
      "                src_mask: Tensor,\n",
      "                tgt_mask: Tensor,\n",
      "                src_padding_mask: Tensor,\n",
      "                tgt_padding_mask: Tensor,\n",
      "                memory_key_padding_mask: Tensor):\n",
      "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
      "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
      "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
      "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
      "        return self.generator(outs)\n",
      "```\n",
      "\n",
      "This code snippet defines the `Seq2SeqTransformer` class, which is used to build a sequence-to-sequence transformer model for machine translation tasks. The class initializes the transformer model with the specified parameters and defines the forward method to perform the transformation process.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"Show the code that built the Seq2SeqTransformer\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8moQ46ZmrTU",
    "outputId": "8cf24734-5165-47d9-af7b-89652ea26222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformer architecture used in the assignment is a Seq2SeqTransformer model. This model consists of several key components, including the Transformer module, positional encoding, token embedding, and a linear generator layer. The Transformer module is responsible for processing the input source and target embeddings using multi-head self-attention mechanisms and feedforward neural networks. It consists of encoder and decoder layers with specified parameters such as the number of layers, feedforward dimension, and dropout rate.\n",
      "\n",
      "The positional encoding is added to the token embeddings to introduce a notion of word order in the model. It helps the model capture the sequential information of the input sentences. The token embedding layers for both the source and target languages are used to convert token indices into dense vectors suitable for processing by the Transformer module.\n",
      "\n",
      "Additionally, the linear generator layer is responsible for mapping the output embeddings from the Transformer module to the target vocabulary size, enabling the generation of translated sentences. Overall, the Seq2SeqTransformer model leverages the transformer architecture to perform the task of translating German sentences into English, with potential improvements suggested for enhancing model performance through training on larger and more diverse datasets.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"Can you talk about the transformer architecture in the Assignment?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tmf9qV7GnBW-",
    "outputId": "97501195-bcd4-46f8-d92a-a84417f255d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the assignment involved training a model on the original German-to-English training set using PyTorch. The assignment focused on using the Multi30k dataset from the torchtext library, implementing a Seq2SeqTransformer model for the translation task, and addressing techniques to enhance model performance such as training on a larger and more diverse dataset.\n",
      "\n",
      "However, there is no specific mention of transfer learning being used in the assignment within the provided context. Transfer learning typically involves utilizing pre-trained models on a related task to improve performance on a new task, but without explicit information in the context regarding the use of transfer learning, it is not possible to confirm whether it was employed in this particular assignment.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"Did they use any Transfer learning in the assignment? If yes, where?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhQNQS4EnZ9T",
    "outputId": "6129276f-2ce5-4f68-ca5f-266054ba8629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the input and prediction for the translation task using the Seq2SeqTransformer model in the German-to-English translation assignment are as follows:\n",
      "\n",
      "Input: \"Eine Gruppe von Menschen steht vor einem Iglu.\"\n",
      "Prediction: \"A group of people standing in front an igloo.\"\n",
      "\n",
      "Input: \"Das Auto steht vor dem Haus.\"\n",
      "Prediction: \"The car is standing in front of the house.\"\n",
      "\n",
      "Input: \"Die Katze liegt auf dem Sofa.\"\n",
      "Prediction: \"The cat is laying on the couch.\"\n",
      "\n",
      "These are examples of input sentences in German and their corresponding translations predicted by the Seq2SeqTransformer model trained on the German-to-English training set.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"Show the input and the prediction\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9A8xHh5w4wc"
   },
   "source": [
    "We get a much more informed response that includes several items missing in the previous non-RAG response, such as \"red-teaming\", \"iterative evaluations\", and the intention of the researchers to share this research to help \"improve their safety, promoting responsible development in the field\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbqygLLrqrpQ"
   },
   "source": [
    "**Observations and Limitations:**\n",
    "- While the RAG model offered more comprehensive insights derived from textual content, the LLM struggled to generate code examples based on embeddings.\n",
    "- PDF conversion of notebooks often includes special characters, whose removal is likely to enhance response quality.\n",
    "- Chunking format ensures proper data loading and ingestion.\n",
    "- Adding prompts and responses to messages expands content, enabling the chatbot to engage in conversation.\n",
    "- Adjusting system parameters influences the results of the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPrsKLj7w4wc"
   },
   "source": [
    "Delete the index to save resources and not be charged for non-use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "E8LSIHSGw4wc"
   },
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p6sE9-KBxdL"
   },
   "source": [
    "Reference:\n",
    "\n",
    "1. DATA_255 RAG Chatbot with LangChain demo.\n",
    "\n",
    "2. [OpenAI API key](https://platform.openai.com/account/api-keys) and [Pinecone API key](https://app.pinecone.io)\n",
    "\n",
    "3. [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings/use-cases)\n",
    "\n",
    "4. [RAGs with OpenAI](https://cookbook.openai.com/examples/parse_pdf_docs_for_rag)\n",
    "\n",
    "5. https://github.com/jrgosalvez/data255_DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtiAQmWdw4wd"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "147db994a0ef40e8b156ec8efc282216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e6f67823ccbe4575b67ef366559ef115",
       "IPY_MODEL_e2251259cba249868868886d109ae35c",
       "IPY_MODEL_1ed3b66d5f8948b79d70985d2da9bba0"
      ],
      "layout": "IPY_MODEL_9918148e66a640e98ba43fdc47b8f91d"
     }
    },
    "14f46db244884f08b5fde46fa48fa227": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ed3b66d5f8948b79d70985d2da9bba0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3f212423af74648bb9fb9ded43a20d9",
      "placeholder": "​",
      "style": "IPY_MODEL_14f46db244884f08b5fde46fa48fa227",
      "value": " 2/2 [00:10&lt;00:00,  4.97s/it]"
     }
    },
    "47036c7a921a410e84e5ab9f538e162f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ad55880bd4443dd8a87981dea80b4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5facec2d0cc14519aa26f493dc202160": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9918148e66a640e98ba43fdc47b8f91d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3f212423af74648bb9fb9ded43a20d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c90b082e6ab64e9d9febe55ef7606f4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e2251259cba249868868886d109ae35c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5facec2d0cc14519aa26f493dc202160",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4ad55880bd4443dd8a87981dea80b4d6",
      "value": 2
     }
    },
    "e6f67823ccbe4575b67ef366559ef115": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47036c7a921a410e84e5ab9f538e162f",
      "placeholder": "​",
      "style": "IPY_MODEL_c90b082e6ab64e9d9febe55ef7606f4a",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
