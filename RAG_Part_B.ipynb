{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API KEY\"\n",
    "# Set your Pinecone API key here\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pld0aGdIBVhZ"
   },
   "source": [
    "# Homework 12 - Part B: Custom Data Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3xE-W5ew4wU"
   },
   "source": [
    "### Part B Goal\n",
    "\n",
    "Build a chatbot to answer questions based on custom data from multiple documents using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using **R**etrieval **A**ugmented **G**eneration (RAG).\n",
    "\n",
    "The chatbot will save the conversation in memory such that it can expand on the conversation based on the past and summarize the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg8_mMrYw4wV"
   },
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnRFn2gdw4wV"
   },
   "source": [
    "Install the following Python libraries:\n",
    "\n",
    "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
    "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
    "- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n",
    "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
    "\n",
    "**NOTE**: *OpenAI dataloaders will not load locally for on-prem devices easily. To simplify the use of these loaders, it is recommended to use an online notebook such as CoLab.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CflZ3e82w4wV",
    "outputId": "6725223f-1822-4d12-9d23-04e54777eb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.0.354 \\\n",
    "    openai==1.6.1 \\\n",
    "    datasets==2.10.1 \\\n",
    "    pinecone-client==3.1.0 \\\n",
    "    tiktoken==0.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUH6ontXw4wV"
   },
   "source": [
    "### BACKGROUND: Building a Chatbot (no RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAps57bQw4wW"
   },
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hF4HeBjw4wW",
    "outputId": "b004d426-d070-40da-9798-629f41162aff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kSbM7Phiw4wW"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to know when Lewis Hamilton is moving to Ferrari.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6TnpBRAw4wW"
   },
   "source": [
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHFShuVMw4wW",
    "outputId": "586e76ac-152c-4da2-acb0-c8ebfe214289"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I don't have that information. As of now, there have been no official announcements regarding Lewis Hamilton moving to Ferrari. It's best to stay updated through reliable sources for any news regarding this matter.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7LoZQClw4wW"
   },
   "source": [
    "In response we get another AI message object. We can print it more clearly like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DzT8TK29w4wX",
    "outputId": "04ae716d-463a-4204-e5b0-57599a097762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have that information. As of now, there have been no official announcements regarding Lewis Hamilton moving to Ferrari. It's best to stay updated through reliable sources for any news regarding this matter.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXb4dFHXw4wX"
   },
   "source": [
    "### Stringing Messages for a Conversation\n",
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UZVj7P5w4wX",
    "outputId": "c154681b-ac4d-4d7c-a6f9-9648b5218ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of the end of the 2021 Formula 1 season, Lewis Hamilton has won a total of 7 Formula 1 World Championships.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"How many championships has Lewis Hamilton Won?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtSC6WmBw4wX"
   },
   "source": [
    "### Dealing with Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAhStCG9w4wX"
   },
   "source": [
    "We have our chatbot, but as mentioned â€” the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about the new (and very popular) Llama 2 LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vdtjTQebw4wX"
   },
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Who are the drivers lined up for Ferrari in 2025?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zMi7B6LZw4wX",
    "outputId": "2004bc1b-07e9-430f-f918-4750b4d96257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to provide information on future driver lineups as it is subject to change and speculation. It's best to stay updated through official announcements from Ferrari or reliable sources for the most accurate information.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YORp9R_Pw4wX"
   },
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer â€” and this can be very hard to detect.\n",
    "\n",
    "OpenAI have since adjusted the behavior for this particular example as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "30od2wnkw4wX"
   },
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you tell me where will Lewis Hamilton drive next Season in F1?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZOSGe-Kw4wX",
    "outputId": "96a5512e-0bd4-4faa-c101-548029560a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have real-time information on Lewis Hamilton's team for the upcoming Formula 1 season. It's best to follow official announcements from Lewis Hamilton or his team to get the latest updates on his driving plans for the next season.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-JA-ut8w4wY"
   },
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oPwiUmbhBx8",
    "outputId": "8affb733-a82d-4160-e299-766c303bae2f"
   },
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxc1ORMRf3qv",
    "outputId": "67ee6c49-fad3-4a39-8c6f-70972f6a93b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Ferrari\\nF1\\nTeam\\nfor\\n2025\\nThe\\nScuderia\\nFerrari,\\nalso\\nknown\\nas\\nthe\\nFerrari\\nF1\\nteam,\\nis\\nthe\\nracing\\nteam\\nof\\nthe\\niconic\\nItalian\\nluxury\\nsports\\ncar\\nmanufacturer\\nFerrari.\\nThe\\nteam\\nis\\nbased\\nin\\nMaranello,\\nItaly,\\nand\\nhas\\nbeen\\na\\ndominant\\nforce\\nin\\nFormula\\nOne\\nracing\\nsince\\nits\\ninception\\nin\\n1950.\\nTeam\\nManagement\\nTeam\\nPrincipal:\\nFrÃ©dÃ©ric\\nVasseur\\nA\\nFrench\\nengineer\\nand\\nmanager,\\nVasseur\\njoined\\nFerrari\\nin\\n2023,\\nreplacing\\nMattia\\nBinotto.\\nHe\\nhas\\nextensive\\nexperience\\nin\\nF1,\\nhaving\\nworked\\nwith\\nteams\\nlike\\nRenault,\\nToyota,\\nand\\nAlfa\\nRomeo.\\nTechnical\\nDirector:\\nEnrico\\nCardile\\nand\\nEnrico\\nGualtieri\\nCardile\\nis\\nresponsible\\nfor\\nthe\\ncar's\\noverall\\ndesign\\nand\\ndevelopment.\\nGualtieri\\noversees\\nthe\\npower\\nunit\\nand\\ntransmission.\\nDriver\\nLineup\\nfor\\n2025\\nCharles\\nLeclerc\\n(Monaco)\\nContract:\\n2025-2027\\nLeclerc\\njoined\\nFerrari\\nin\\n2019\\nand\\nhas\\nbeen\\na\\nkey\\ndriver\\nfor\\nthe\\nteam.\\nHe\\nhas\\nwon\\nseveral\\nraces\\nand\\nhas\\nbeen\\na\\nconsistent\\npoints\\nscorer.\\nLewis\\nHamilton\\n(United\\nKingdom)\\nContract:\\n2025-2027\\nHamilton\\njoins\\nFerrari\\nafter\\na\\nhighly\\nsuccessful\\n12-year\\nstint\\nwith\\nMercedes.\\nHe\\nis\\na\\nseven-time\\nWorld\\nChampion\\nand\\none\\nof\\nthe\\nmost\\nsuccessful\\nF1\\ndrivers\\nin\\nhistory.\\nOther\\nKey\\nPersonnel\\nLaurent\\nMekies:\\nSporting\\nDirector\\nRiccardo\\nAdami:\\nHead\\nof\\nVehicle\\nOperations\\nEnrico\\nGualtieri:\\nHead\\nof\\nPower\\nUnit\\nFerrari\\nF1\\nTeam\\nHistory\\nFerrari\\nis\\nthe\\noldest\\nand\\nmost\\nsuccessful\\nteam\\nin\\nFormula\\nOne,\\nwith\\n16\\nWorld\\nChampionships\\nand\\nover\\n250\\nrace\\nwins.\\nThe\\nteam\\nhas\\na\\nrich\\nhistory,\\nhaving\\nbeen\\nfounded\\nby\\nEnzo\\nFerrari\\nin\\n1947.\\nOver\\nthe\\nyears,\\nFerrari\\nhas\\nfielded\\nsome\\nof\\nthe\\ngreatest\\ndrivers\\nin\\nF1\\nhistory,\\nincluding\\nAlberto\\nAscari,\\nJuan\\nManuel\\nFangio,\\nNiki\\nLauda,\\nand\\nMichael\\nSchumacher.\\nFerrari\\nF1\\nTeam\\nCar\\nThe\\nFerrari\\nF1\\nteam\\ncar\\nfor\\n2025\\nis\\nthe\\nSF-24,\\ndesigned\\nby\\nEnrico\\nCardile\\nand\\nhis\\nteam.\\nThe\\ncar\\nfeatures\\na\\n1.6-liter\\nturbocharged\\nV6\\nengine,\\nproducing\\nover\\n1,000\\nhorsepower.\\nThe\\nSF-24\\nalso\\nfeatures\\nadvanced\\naerodynamics,\\nincluding\\ncomplex\\nwing\\ndesigns\\nand\\na\\nsophisticated\\ndrag\\nreduction\\nsystem.\\nChallenges\\nand\\nExpectations\\nThe\\nFerrari\\nF1\\nteam\\nfaces\\nstiff\\ncompetition\\nfrom\\nother\\ntop\\nteams\\nlike\\nMercedes,\\nRed\\nBull,\\nand\\nMcLaren.\\nThe\\nteam\\nhas\\nbeen\\nworking\\nhard\\nto\\nimprove\\nits\\ncar\\nand\\noperations,\\nand\\nthe\\naddition\\nof\\nLewis\\nHamilton\\nis\\nexpected\\nto\\nbring\\na\\nnew\\nlevel\\nof\\nexpertise\\nand\\ncompetitiveness.\\nThe\\nteam's\\ngoal\\nfor\\n2025\\nis\\nto\\nwin\\nthe\\nConstructors'\\nChampionship\\nand\\nsupport\\nits\\ndrivers\\nin\\ntheir\\nquest\\nfor\\nthe\\nDrivers'\\nChampionship.\\nConclusion\\nThe\\nFerrari\\nF1\\nteam\\nis\\na\\nlegendary\\noutfit\\nwith\\na\\nrich\\nhistory\\nand\\na\\npassionate\\nfan\\nbase.\\nWith\\na\\nstrong\\nteam\\nmanagement,\\ntalented\\ndrivers,\\nand\\na\\ncompetitive\\ncar,\\nthe\\nteam\\nis\\nwell-positioned\\nto\", metadata={'source': 'Ferrari F1 Team for 2025.pdf', 'page': 0}), Document(page_content='succeed\\nin\\nthe\\n2025\\nF1\\nseason.\\nAs\\nthe\\nteam\\ncontinues\\nto\\nevolve\\nand\\nimprove,\\nfans\\naround\\nthe\\nworld\\nwill\\nbe\\neagerly\\nwatching\\nto\\nsee\\nif\\nFerrari\\ncan\\nreclaim\\nits\\nposition\\nat\\nthe\\ntop\\nof\\nthe\\nF1\\npodium.', metadata={'source': 'Ferrari F1 Team for 2025.pdf', 'page': 1})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "#load pdf files\n",
    "loader = PyPDFLoader('Ferrari F1 Team for 2025.pdf')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdORRTUoi3vp",
    "outputId": "bae499da-a1dd-4bb6-c552-67b8d39ebc62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split text data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(data)\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3t1UdQztjFp6",
    "outputId": "284a9ad8-8f2e-4a17-cced-a234dc730f01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"from\\nother\\ntop\\nteams\\nlike\\nMercedes,\\nRed\\nBull,\\nand\\nMcLaren.\\nThe\\nteam\\nhas\\nbeen\\nworking\\nhard\\nto\\nimprove\\nits\\ncar\\nand\\noperations,\\nand\\nthe\\naddition\\nof\\nLewis\\nHamilton\\nis\\nexpected\\nto\\nbring\\na\\nnew\\nlevel\\nof\\nexpertise\\nand\\ncompetitiveness.\\nThe\\nteam's\\ngoal\\nfor\\n2025\\nis\\nto\\nwin\\nthe\\nConstructors'\\nChampionship\\nand\\nsupport\\nits\\ndrivers\\nin\\ntheir\\nquest\\nfor\\nthe\\nDrivers'\\nChampionship.\\nConclusion\\nThe\\nFerrari\\nF1\\nteam\\nis\\na\\nlegendary\\noutfit\\nwith\\na\\nrich\\nhistory\\nand\\na\\npassionate\\nfan\\nbase.\\nWith\\na\\nstrong\\nteam\\nmanagement,\\ntalented\\ndrivers,\\nand\\na\\ncompetitive\\ncar,\\nthe\\nteam\\nis\\nwell-positioned\\nto\", metadata={'source': 'Ferrari F1 Team for 2025.pdf', 'page': 0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the chunks\n",
    "text_chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyeGKo8-jqAf",
    "outputId": "04d4d44e-f87c-4d89-9a51-05656717d48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doi': '', 'chunk-id': '3', 'chunk': Document(page_content='succeed\\nin\\nthe\\n2025\\nF1\\nseason.\\nAs\\nthe\\nteam\\ncontinues\\nto\\nevolve\\nand\\nimprove,\\nfans\\naround\\nthe\\nworld\\nwill\\nbe\\neagerly\\nwatching\\nto\\nsee\\nif\\nFerrari\\ncan\\nreclaim\\nits\\nposition\\nat\\nthe\\ntop\\nof\\nthe\\nF1\\npodium.', metadata={'source': 'Ferrari F1 Team for 2025.pdf', 'page': 1}), 'id': '', 'title': '', 'summary': '', 'source': '', 'authors': [], 'categories': [], 'comment': '', 'journal_ref': None, 'primary_category': '', 'published': '', 'updated': '', 'references': []}\n"
     ]
    }
   ],
   "source": [
    "# reformat chunks to improve vectorization; match 'jamescalam/llama-2-arxiv-papers-chunked' format sourced from Llama 2 ArXiv papers on huggingface\n",
    "dataset = []\n",
    "\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    dataset.append({\n",
    "        'doi': '',  # you can add a DOI here if available\n",
    "        'chunk-id': str(i),\n",
    "        'chunk': chunk,\n",
    "        'id': '',  # you can add an ID here if available\n",
    "        'title': '',  # you can add a title here if available\n",
    "        'summary': '',  # you can add a summary here if available\n",
    "        'source': '',  # you can add a source here if available\n",
    "        'authors': [],  # you can add authors here if available\n",
    "        'categories': [],  # you can add categories here if available\n",
    "        'comment': '',  # you can add a comment here if available\n",
    "        'journal_ref': None,  # you can add a journal reference here if available\n",
    "        'primary_category': '',  # you can add a primary category here if available\n",
    "        'published': '',  # you can add a published date here if available\n",
    "        'updated': '',  # you can add an updated date here if available\n",
    "        'references': []  # you can add references here if available\n",
    "    })\n",
    "\n",
    "print(dataset[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo1AYqcZw4wY"
   },
   "source": [
    "#### Dataset Overview\n",
    "\n",
    "The dataset used are PDFs samples of my (Silksong Gosalvez's) Deep Learning homeworks.\n",
    "\n",
    "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Silksong the game without example data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq3-dxkGw4wY"
   },
   "source": [
    "### Task 4: Building the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsYU27hBw4wY"
   },
   "source": [
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our connection to Pinecone, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GxIvcXXOw4wb"
   },
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize connection to Pinecone using the environment variable\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kcal_JEgw4wb"
   },
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uTLMWZPAw4wb"
   },
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSxNjSjLw4wb"
   },
   "source": [
    "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULRvhj4aw4wb",
    "outputId": "4656a646-785a-4085-b4ee-d951038019c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'llama-2-rag'\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of ada 002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jae7TDz5w4wb"
   },
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model â€” we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCQ-XA0Vw4wb",
    "outputId": "4a695a64-e72a-43ec-8c8b-b803e0c8ae7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONTjqjWww4wb"
   },
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugECVXqDw4wb",
    "outputId": "6f928d6c-2eb0-4945-de1a-ae9245488585"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f82zGPmIw4wb"
   },
   "source": [
    "From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches.\n",
    "\n",
    "**NOTE**: *ensure that chunks are strings and ensure that they are correctly assigned to metadata (do this with the .page_content method)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "5b9bcff7e778481abb5a2f04490bc10a",
      "8cc2c2c2c6a94a6e945457ae4defdee9",
      "fd78bca43c454152b60a0527f37250b5",
      "8348a25a6f714de7aeedf93ccbbe270b",
      "7bed3d00469049d1bb7dd74cc37e6ef1",
      "fb9b695c9a9442d3824dfdf913f9ddb2",
      "db0541c3e7e54cdbad561f78fa85ff79",
      "18d47641d1bb41b98151c59164a8a856",
      "b2c9225eb98d4292b1ac54331cd317b2",
      "6c72d691fe9841288bfa3c03a6f27353",
      "eb1a5c78839746ed8b28c861216d5705"
     ]
    },
    "id": "AtgH_iMuw4wb",
    "outputId": "03738a87-93d9-415c-e6b0-8dd0b814616c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = pd.DataFrame(dataset) # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [str(x['chunk']) for _, x in batch.iterrows()]\n",
    "\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'].page_content,\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9p8OBkyw4wc"
   },
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3QdqBijw4wc",
    "outputId": "5c1bab86-9fea-41a7-9af3-49fc6317b627"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqqVoZnkw4wc"
   },
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcm-dCPCw4wc"
   },
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJPGGyXSw4wc"
   },
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4MPMJmtCw4wc",
    "outputId": "7fd3e064-ec18-4e6e-d220-f6bdb5781fc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.vectorstores.pinecone.Pinecone` was deprecated in langchain-community 0.0.18 and will be removed in 0.2.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zrlwGYUw4wc"
   },
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Silksong's prior deep learning homeworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyShvreew4wc",
    "outputId": "533f2f01-bd57-4ef7-d213-1c071d6d18ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='12-year\\nstint\\nwith\\nMercedes.\\nHe\\nis\\na\\nseven-time\\nWorld\\nChampion\\nand\\none\\nof\\nthe\\nmost\\nsuccessful\\nF1\\ndrivers\\nin\\nhistory.\\nOther\\nKey\\nPersonnel\\nLaurent\\nMekies:\\nSporting\\nDirector\\nRiccardo\\nAdami:\\nHead\\nof\\nVehicle\\nOperations\\nEnrico\\nGualtieri:\\nHead\\nof\\nPower\\nUnit\\nFerrari\\nF1\\nTeam\\nHistory\\nFerrari\\nis\\nthe\\noldest\\nand\\nmost\\nsuccessful\\nteam\\nin\\nFormula\\nOne,\\nwith\\n16\\nWorld\\nChampionships\\nand\\nover\\n250\\nrace\\nwins.\\nThe\\nteam\\nhas\\na\\nrich\\nhistory,\\nhaving\\nbeen\\nfounded\\nby\\nEnzo\\nFerrari\\nin\\n1947.\\nOver\\nthe\\nyears,\\nFerrari\\nhas\\nfielded\\nsome\\nof\\nthe\\ngreatest\\ndrivers\\nin\\nF1\\nhistory,\\nincluding\\nAlberto\\nAscari,\\nJuan\\nManuel\\nFangio,\\nNiki\\nLauda,\\nand\\nMichael\\nSchumacher.\\nFerrari\\nF1\\nTeam\\nCar\\nThe\\nFerrari\\nF1\\nteam\\ncar\\nfor\\n2025\\nis\\nthe\\nSF-24,\\ndesigned\\nby\\nEnrico\\nCardile\\nand\\nhis\\nteam.\\nThe\\ncar\\nfeatures\\na\\n1.6-liter\\nturbocharged\\nV6\\nengine,\\nproducing\\nover\\n1,000\\nhorsepower.\\nThe\\nSF-24\\nalso\\nfeatures\\nadvanced\\naerodynamics,\\nincluding\\ncomplex\\nwing\\ndesigns\\nand\\na\\nsophisticated\\ndrag\\nreduction\\nsystem.\\nChallenges\\nand\\nExpectations\\nThe\\nFerrari\\nF1\\nteam\\nfaces\\nstiff\\ncompetition\\nfrom\\nother', metadata={'source': '', 'title': ''}),\n",
       " Document(page_content=\"Ferrari\\nF1\\nTeam\\nfor\\n2025\\nThe\\nScuderia\\nFerrari,\\nalso\\nknown\\nas\\nthe\\nFerrari\\nF1\\nteam,\\nis\\nthe\\nracing\\nteam\\nof\\nthe\\niconic\\nItalian\\nluxury\\nsports\\ncar\\nmanufacturer\\nFerrari.\\nThe\\nteam\\nis\\nbased\\nin\\nMaranello,\\nItaly,\\nand\\nhas\\nbeen\\na\\ndominant\\nforce\\nin\\nFormula\\nOne\\nracing\\nsince\\nits\\ninception\\nin\\n1950.\\nTeam\\nManagement\\nTeam\\nPrincipal:\\nFrÃ©dÃ©ric\\nVasseur\\nA\\nFrench\\nengineer\\nand\\nmanager,\\nVasseur\\njoined\\nFerrari\\nin\\n2023,\\nreplacing\\nMattia\\nBinotto.\\nHe\\nhas\\nextensive\\nexperience\\nin\\nF1,\\nhaving\\nworked\\nwith\\nteams\\nlike\\nRenault,\\nToyota,\\nand\\nAlfa\\nRomeo.\\nTechnical\\nDirector:\\nEnrico\\nCardile\\nand\\nEnrico\\nGualtieri\\nCardile\\nis\\nresponsible\\nfor\\nthe\\ncar's\\noverall\\ndesign\\nand\\ndevelopment.\\nGualtieri\\noversees\\nthe\\npower\\nunit\\nand\\ntransmission.\\nDriver\\nLineup\\nfor\\n2025\\nCharles\\nLeclerc\\n(Monaco)\\nContract:\\n2025-2027\\nLeclerc\\njoined\\nFerrari\\nin\\n2019\\nand\\nhas\\nbeen\\na\\nkey\\ndriver\\nfor\\nthe\\nteam.\\nHe\\nhas\\nwon\\nseveral\\nraces\\nand\\nhas\\nbeen\\na\\nconsistent\\npoints\\nscorer.\\nLewis\\nHamilton\\n(United\\nKingdom)\\nContract:\\n2025-2027\\nHamilton\\njoins\\nFerrari\\nafter\\na\\nhighly\\nsuccessful\\n12-year\\nstint\\nwith\", metadata={'source': '', 'title': ''}),\n",
       " Document(page_content=\"from\\nother\\ntop\\nteams\\nlike\\nMercedes,\\nRed\\nBull,\\nand\\nMcLaren.\\nThe\\nteam\\nhas\\nbeen\\nworking\\nhard\\nto\\nimprove\\nits\\ncar\\nand\\noperations,\\nand\\nthe\\naddition\\nof\\nLewis\\nHamilton\\nis\\nexpected\\nto\\nbring\\na\\nnew\\nlevel\\nof\\nexpertise\\nand\\ncompetitiveness.\\nThe\\nteam's\\ngoal\\nfor\\n2025\\nis\\nto\\nwin\\nthe\\nConstructors'\\nChampionship\\nand\\nsupport\\nits\\ndrivers\\nin\\ntheir\\nquest\\nfor\\nthe\\nDrivers'\\nChampionship.\\nConclusion\\nThe\\nFerrari\\nF1\\nteam\\nis\\na\\nlegendary\\noutfit\\nwith\\na\\nrich\\nhistory\\nand\\na\\npassionate\\nfan\\nbase.\\nWith\\na\\nstrong\\nteam\\nmanagement,\\ntalented\\ndrivers,\\nand\\na\\ncompetitive\\ncar,\\nthe\\nteam\\nis\\nwell-positioned\\nto\", metadata={'source': '', 'title': ''})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Did Lewis Hamilton sign with Ferrari?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huzLImnKw4wc"
   },
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Rp5NBaqfw4wc"
   },
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2-7QCTew4wc"
   },
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mftcb16Cw4wc",
    "outputId": "d2ecc08d-06ed-4cfb-e35e-be48a14e8c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    12-year\n",
      "stint\n",
      "with\n",
      "Mercedes.\n",
      "He\n",
      "is\n",
      "a\n",
      "seven-time\n",
      "World\n",
      "Champion\n",
      "and\n",
      "one\n",
      "of\n",
      "the\n",
      "most\n",
      "successful\n",
      "F1\n",
      "drivers\n",
      "in\n",
      "history.\n",
      "Other\n",
      "Key\n",
      "Personnel\n",
      "Laurent\n",
      "Mekies:\n",
      "Sporting\n",
      "Director\n",
      "Riccardo\n",
      "Adami:\n",
      "Head\n",
      "of\n",
      "Vehicle\n",
      "Operations\n",
      "Enrico\n",
      "Gualtieri:\n",
      "Head\n",
      "of\n",
      "Power\n",
      "Unit\n",
      "Ferrari\n",
      "F1\n",
      "Team\n",
      "History\n",
      "Ferrari\n",
      "is\n",
      "the\n",
      "oldest\n",
      "and\n",
      "most\n",
      "successful\n",
      "team\n",
      "in\n",
      "Formula\n",
      "One,\n",
      "with\n",
      "16\n",
      "World\n",
      "Championships\n",
      "and\n",
      "over\n",
      "250\n",
      "race\n",
      "wins.\n",
      "The\n",
      "team\n",
      "has\n",
      "a\n",
      "rich\n",
      "history,\n",
      "having\n",
      "been\n",
      "founded\n",
      "by\n",
      "Enzo\n",
      "Ferrari\n",
      "in\n",
      "1947.\n",
      "Over\n",
      "the\n",
      "years,\n",
      "Ferrari\n",
      "has\n",
      "fielded\n",
      "some\n",
      "of\n",
      "the\n",
      "greatest\n",
      "drivers\n",
      "in\n",
      "F1\n",
      "history,\n",
      "including\n",
      "Alberto\n",
      "Ascari,\n",
      "Juan\n",
      "Manuel\n",
      "Fangio,\n",
      "Niki\n",
      "Lauda,\n",
      "and\n",
      "Michael\n",
      "Schumacher.\n",
      "Ferrari\n",
      "F1\n",
      "Team\n",
      "Car\n",
      "The\n",
      "Ferrari\n",
      "F1\n",
      "team\n",
      "car\n",
      "for\n",
      "2025\n",
      "is\n",
      "the\n",
      "SF-24,\n",
      "designed\n",
      "by\n",
      "Enrico\n",
      "Cardile\n",
      "and\n",
      "his\n",
      "team.\n",
      "The\n",
      "car\n",
      "features\n",
      "a\n",
      "1.6-liter\n",
      "turbocharged\n",
      "V6\n",
      "engine,\n",
      "producing\n",
      "over\n",
      "1,000\n",
      "horsepower.\n",
      "The\n",
      "SF-24\n",
      "also\n",
      "features\n",
      "advanced\n",
      "aerodynamics,\n",
      "including\n",
      "complex\n",
      "wing\n",
      "designs\n",
      "and\n",
      "a\n",
      "sophisticated\n",
      "drag\n",
      "reduction\n",
      "system.\n",
      "Challenges\n",
      "and\n",
      "Expectations\n",
      "The\n",
      "Ferrari\n",
      "F1\n",
      "team\n",
      "faces\n",
      "stiff\n",
      "competition\n",
      "from\n",
      "other\n",
      "Ferrari\n",
      "F1\n",
      "Team\n",
      "for\n",
      "2025\n",
      "The\n",
      "Scuderia\n",
      "Ferrari,\n",
      "also\n",
      "known\n",
      "as\n",
      "the\n",
      "Ferrari\n",
      "F1\n",
      "team,\n",
      "is\n",
      "the\n",
      "racing\n",
      "team\n",
      "of\n",
      "the\n",
      "iconic\n",
      "Italian\n",
      "luxury\n",
      "sports\n",
      "car\n",
      "manufacturer\n",
      "Ferrari.\n",
      "The\n",
      "team\n",
      "is\n",
      "based\n",
      "in\n",
      "Maranello,\n",
      "Italy,\n",
      "and\n",
      "has\n",
      "been\n",
      "a\n",
      "dominant\n",
      "force\n",
      "in\n",
      "Formula\n",
      "One\n",
      "racing\n",
      "since\n",
      "its\n",
      "inception\n",
      "in\n",
      "1950.\n",
      "Team\n",
      "Management\n",
      "Team\n",
      "Principal:\n",
      "FrÃ©dÃ©ric\n",
      "Vasseur\n",
      "A\n",
      "French\n",
      "engineer\n",
      "and\n",
      "manager,\n",
      "Vasseur\n",
      "joined\n",
      "Ferrari\n",
      "in\n",
      "2023,\n",
      "replacing\n",
      "Mattia\n",
      "Binotto.\n",
      "He\n",
      "has\n",
      "extensive\n",
      "experience\n",
      "in\n",
      "F1,\n",
      "having\n",
      "worked\n",
      "with\n",
      "teams\n",
      "like\n",
      "Renault,\n",
      "Toyota,\n",
      "and\n",
      "Alfa\n",
      "Romeo.\n",
      "Technical\n",
      "Director:\n",
      "Enrico\n",
      "Cardile\n",
      "and\n",
      "Enrico\n",
      "Gualtieri\n",
      "Cardile\n",
      "is\n",
      "responsible\n",
      "for\n",
      "the\n",
      "car's\n",
      "overall\n",
      "design\n",
      "and\n",
      "development.\n",
      "Gualtieri\n",
      "oversees\n",
      "the\n",
      "power\n",
      "unit\n",
      "and\n",
      "transmission.\n",
      "Driver\n",
      "Lineup\n",
      "for\n",
      "2025\n",
      "Charles\n",
      "Leclerc\n",
      "(Monaco)\n",
      "Contract:\n",
      "2025-2027\n",
      "Leclerc\n",
      "joined\n",
      "Ferrari\n",
      "in\n",
      "2019\n",
      "and\n",
      "has\n",
      "been\n",
      "a\n",
      "key\n",
      "driver\n",
      "for\n",
      "the\n",
      "team.\n",
      "He\n",
      "has\n",
      "won\n",
      "several\n",
      "races\n",
      "and\n",
      "has\n",
      "been\n",
      "a\n",
      "consistent\n",
      "points\n",
      "scorer.\n",
      "Lewis\n",
      "Hamilton\n",
      "(United\n",
      "Kingdom)\n",
      "Contract:\n",
      "2025-2027\n",
      "Hamilton\n",
      "joins\n",
      "Ferrari\n",
      "after\n",
      "a\n",
      "highly\n",
      "successful\n",
      "12-year\n",
      "stint\n",
      "with\n",
      "from\n",
      "other\n",
      "top\n",
      "teams\n",
      "like\n",
      "Mercedes,\n",
      "Red\n",
      "Bull,\n",
      "and\n",
      "McLaren.\n",
      "The\n",
      "team\n",
      "has\n",
      "been\n",
      "working\n",
      "hard\n",
      "to\n",
      "improve\n",
      "its\n",
      "car\n",
      "and\n",
      "operations,\n",
      "and\n",
      "the\n",
      "addition\n",
      "of\n",
      "Lewis\n",
      "Hamilton\n",
      "is\n",
      "expected\n",
      "to\n",
      "bring\n",
      "a\n",
      "new\n",
      "level\n",
      "of\n",
      "expertise\n",
      "and\n",
      "competitiveness.\n",
      "The\n",
      "team's\n",
      "goal\n",
      "for\n",
      "2025\n",
      "is\n",
      "to\n",
      "win\n",
      "the\n",
      "Constructors'\n",
      "Championship\n",
      "and\n",
      "support\n",
      "its\n",
      "drivers\n",
      "in\n",
      "their\n",
      "quest\n",
      "for\n",
      "the\n",
      "Drivers'\n",
      "Championship.\n",
      "Conclusion\n",
      "The\n",
      "Ferrari\n",
      "F1\n",
      "team\n",
      "is\n",
      "a\n",
      "legendary\n",
      "outfit\n",
      "with\n",
      "a\n",
      "rich\n",
      "history\n",
      "and\n",
      "a\n",
      "passionate\n",
      "fan\n",
      "base.\n",
      "With\n",
      "a\n",
      "strong\n",
      "team\n",
      "management,\n",
      "talented\n",
      "drivers,\n",
      "and\n",
      "a\n",
      "competitive\n",
      "car,\n",
      "the\n",
      "team\n",
      "is\n",
      "well-positioned\n",
      "to\n",
      "\n",
      "    Query: Did Lewis Hamilton sign with Ferrari?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZI3-CdZw4wc"
   },
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3j-9JMRWw4wc",
    "outputId": "ed709ed8-01ed-4735-a791-07fb36f1ffdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Lewis Hamilton signed with Ferrari for the 2025-2027 seasons after a successful 12-year stint with Mercedes.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwWgr78fw4wc"
   },
   "source": [
    "We can continue with more questions about Silksong's prior deep learning homeworks. Let's try _without_ RAG first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBuJCW1ew4wc",
    "outputId": "6ea4402c-ce86-4490-e58a-f1e3356bdc71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Team Principal of Scuderia Ferrari is FrÃ©dÃ©ric Vasseur.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"Who is the Team boss of Scuderia?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ts3kGD0w4wc"
   },
   "source": [
    "The chatbot is able to respond about Silksong's prior deep learning homeworks thanks to it's conversational history stored in `messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuWbaJAIw4wc",
    "outputId": "89d1e80d-7c7c-4769-b3c6-fc16f0b11209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrÃ©dÃ©ric Vasseur is the Team Principal of the Ferrari F1 team. He is a French engineer and manager who joined Ferrari in 2023, replacing Mattia Binotto. Vasseur has extensive experience in Formula 1, having worked with teams like Renault, Toyota, and Alfa Romeo. As Team Principal, he plays a crucial role in overseeing the team's operations and strategy to help Ferrari achieve success on the track.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"What can you say about FrÃ©dÃ©ric Vasseur ?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZq8hZQemR15",
    "outputId": "c94f6b27-3672-4e6c-fa3a-03491ddec479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fans should pay attention to the 2025 F1 season to see if Ferrari can reclaim its position at the top of the podium with their new driver lineup and car.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"What date should fans pay attention to?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8moQ46ZmrTU",
    "outputId": "1f6f42a3-3e4f-49be-9c87-c97ff666d6b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Scuderia Ferrari horse is typically depicted in black on a yellow background.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"What color is the Scuderia Ferrari Horse?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tmf9qV7GnBW-",
    "outputId": "a3074c07-5dc5-4fc6-91de-2287adce2812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Leclerc is from Monaco, and Monaco is a country located in Europe.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"Which country is Charles Leclerc from and do you know which continent is that country from?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zyI_DCfn5le",
    "outputId": "e05e4791-283d-4da7-defc-578199812550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ferrari has won 16 World Championships and over 250 race wins in Formula One.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"How many world championships and races has Ferrari won'?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8_FFWCW_AyD",
    "outputId": "5881367b-91a5-4454-847d-e4583d9997e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrico Cardile and his team designed the Ferrari F1 team car for 2025, the SF-24.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"Who designed Ferrari Car for 2025?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhQNQS4EnZ9T",
    "outputId": "e46639b5-929a-404f-d559-8c83438693ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The technical directors of Ferrari are Enrico Cardile and Enrico Gualtieri. Enrico Cardile is responsible for the car's overall design and development, while Enrico Gualtieri oversees the power unit and transmission.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"Who are the technical directors of Ferrari?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vB4V01dF-V-i",
    "outputId": "1c209b87-6462-4ba3-ebf5-9595705ac5df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Lewis Hamilton has joined Ferrari for the 2025-2027 seasons after a successful 12-year stint with Mercedes.\n",
      "- Ferrari F1 team aims to win the Constructors' Championship and support its drivers in their quest for the Drivers' Championship in 2025.\n",
      "- The team is well-positioned with strong team management, talented drivers, and a competitive car for the upcoming F1 season.\n",
      "- Ferrari F1 team, also known as Scuderia Ferrari, has been a dominant force in Formula One racing since its inception in 1950.\n",
      "- FrÃ©dÃ©ric Vasseur is the Team Principal of Ferrari F1 team, and Enrico Cardile and Enrico Gualtieri serve as the Technical Director overseeing car design and power unit, respectively.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"Summarize our chat in bullets.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbqygLLrqrpQ"
   },
   "source": [
    "**`Observations and Limitations of the Large Language Model (LLM)`**\n",
    "\n",
    "*Complexity of PDFs*: The LLM's ability to extract information from PDFs is hindered by the presence of special characters and formatting complexities, resulting in incomplete data capture. For instance, the LLM successfully identified the context written by Jordan Sirani but failed to attribute authorship to him.\n",
    "\n",
    "*Chunking format*: The utilization of chunking format ensures efficient data loading and ingestion, facilitating the processing of large amounts of information.\n",
    "\n",
    "*Prompt and response appending*: The appending of prompts and responses to messages enables the expansion of content, allowing the chatbot to engage in conversational exchanges.\n",
    "\n",
    "*Message saving and conversation recall*: The passing forward of messages enables the chatbot to \"remember\" the conversation, facilitating the drawing of conclusions and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPrsKLj7w4wc"
   },
   "source": [
    "Delete the index to save resources and not be charged for non-use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "E8LSIHSGw4wc"
   },
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtiAQmWdw4wd"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18d47641d1bb41b98151c59164a8a856": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b9bcff7e778481abb5a2f04490bc10a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8cc2c2c2c6a94a6e945457ae4defdee9",
       "IPY_MODEL_fd78bca43c454152b60a0527f37250b5",
       "IPY_MODEL_8348a25a6f714de7aeedf93ccbbe270b"
      ],
      "layout": "IPY_MODEL_7bed3d00469049d1bb7dd74cc37e6ef1"
     }
    },
    "6c72d691fe9841288bfa3c03a6f27353": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bed3d00469049d1bb7dd74cc37e6ef1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8348a25a6f714de7aeedf93ccbbe270b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c72d691fe9841288bfa3c03a6f27353",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_eb1a5c78839746ed8b28c861216d5705",
      "value": "â€‡1/1â€‡[00:03&lt;00:00,â€‡â€‡3.82s/it]"
     }
    },
    "8cc2c2c2c6a94a6e945457ae4defdee9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb9b695c9a9442d3824dfdf913f9ddb2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_db0541c3e7e54cdbad561f78fa85ff79",
      "value": "100%"
     }
    },
    "b2c9225eb98d4292b1ac54331cd317b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "db0541c3e7e54cdbad561f78fa85ff79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb1a5c78839746ed8b28c861216d5705": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb9b695c9a9442d3824dfdf913f9ddb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd78bca43c454152b60a0527f37250b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18d47641d1bb41b98151c59164a8a856",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b2c9225eb98d4292b1ac54331cd317b2",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
